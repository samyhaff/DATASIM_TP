{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrrDvpLFsdza"
   },
   "source": [
    "# Semantic segmentation with Deep Learning\n",
    "Centrale Nantes\n",
    "\n",
    "Diana Mateus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Participants:** Saâd Aziz Alaoui, Yassine Jamoud, Samy Haffoudhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uncs1DSCsdzc"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCycI20csdzd"
   },
   "source": [
    "In many areas of biomedical research, cell counts obtained from images are crucial data for diagnosing patients or for addressing hypotheses about developmental or pathological processes. Although the field changes reapidly, many cell counts are still done manually or with semi-automatic tools. An automatic tool can therefore save time, reduce variability, and improve results. In this notebook we will model the problem in terms of semantic segmentation and approach it by means of deep learning, and more specifically with the U-Net architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O8ZLTAAsdzk"
   },
   "source": [
    "### 1. Load modules and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiYmEPldsdzl"
   },
   "source": [
    "#### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtzrnQ_Usdzl",
    "outputId": "87055bdf-d86b-4ea5-fc7c-016d2db22c23"
   },
   "outputs": [],
   "source": [
    "#4S\n",
    "# Import all the necessary libraries\n",
    "import os\n",
    "import datetime \n",
    "import glob                                           #filename pattern matching\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SKIMAGE\n",
    "import skimage.io                                     #Used for imshow function\n",
    "import skimage.transform                              #Used for resize function\n",
    "import skimage.exposure                               #Used for displaying \n",
    "\n",
    "#Tensorflow, Keras and related modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.layers import add, concatenate\n",
    "\n",
    "\n",
    "#To save and reload models\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "\n",
    "\n",
    "#Sklearn \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Skimage      :', skimage.__version__)\n",
    "print('Scikit-learn :', sklearn.__version__)\n",
    "print('Tensorflow   :', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_e4LIzasdzs"
   },
   "source": [
    "#### 1.2 Import data \n",
    "Download data from\n",
    "\n",
    "- https://box.ec-nantes.fr/index.php/s/X84Bq5NYJ2zJRpR (small dataset to begin with)\n",
    "- https://box.ec-nantes.fr/index.php/s/oXAx37DbpXPDF7T (larger dataset for final tests single zip -> good internet connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTGfl__Lsdzs"
   },
   "source": [
    "#### 1.3 Define the data path\n",
    "Define the data path and change the directory to the defined topDir. Also specify the train and test directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIjJYzPSi5hw",
    "outputId": "d7531712-38cc-43e3-894e-169faa9abfc0"
   },
   "outputs": [],
   "source": [
    "#In case using colab\n",
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx8bDZs0sdzt",
    "outputId": "58f4d13e-37a7-4665-9a66-756dc3e97b21"
   },
   "outputs": [],
   "source": [
    "train_path = \"stage1_train\"#path to training data file/folder\n",
    "test_path = \"stage1_test\" #path to test data file/folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGG4SHF6sdzo"
   },
   "source": [
    "#### 1.4 Define global parameters\n",
    "- Set seed for random values to a fix number (suggested: 42) to ensure that images are associated to the exactly corresponding masks . \n",
    "- Define the desired image size as two global parameters:\n",
    "``` img_height, img_width ```. \n",
    "The defalut image size  is (64,64). You may want to reduce size for faster **debugging** or increase it (e.g. 256,256) for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZzFalvSsdzp"
   },
   "outputs": [],
   "source": [
    "#4S\n",
    "# Set seed values\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Set number of GPUs\n",
    "#num_gpus = 1   #defaults to 1 if one-GPU or one-CPU. If 4 GPUs, set to 4.\n",
    "\n",
    "# Set height (y-axis length) and width (x-axis length) to train model on\n",
    "img_height, img_width = (128,128)  #Default to (256,266), use (None,None) if you do not want to resize imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIQp5Y25sdzv"
   },
   "source": [
    "#### 1.5 Read and preload the input image data\n",
    "Read the data and create a tensor containing all the train and test data as well as their corresponding labels.\n",
    "\n",
    "The following function ``get_images``  \n",
    "- receives as input an image path (can be train or test), and a desired image shape for the output\n",
    "- lists all files in the input image path. \n",
    "- reads each of the files with ``skimage.io.imread``\n",
    "- uses ``skimage.transform.resize`` to directly downsample the images according to the ``img_height, img_width`` parameters.\n",
    "- uses only the first three channels of the images.\n",
    "- returns a numpy array (matrix) . \n",
    "- uses ``%%time`` to display the time spent in loading.\n",
    "\n",
    "Run the function and print the shape and type of the resulting array (e.g. (670, 256, 256, 3) uint8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX8fjX7tsdzw",
    "outputId": "1d04ef7f-e380-4dd9-c0e0-03903e58046b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get training data\n",
    "def get_images(path, output_shape=(None, None)):\n",
    "    '''\n",
    "    Loads images from path/{id}/images/{id}.png into a numpy array\n",
    "    '''\n",
    "    img_paths = ['{0}/{1}/images/{1}.png'.format(path, id) for id in os.listdir(path) if id != \".DS_Store\"]\n",
    "\n",
    "    #X_data = np.array([skimage.io.imread(path)[:,:,:3] for path in img_paths], dtype=np.uint8)  #take only 3 channels/bands\n",
    "\n",
    "    X_data = np.array([skimage.transform.resize(skimage.io.imread(path)[:,:,:3], output_shape=output_shape, mode='constant', preserve_range=True) for path in img_paths], dtype=np.uint8)  #take only 3 channels/bands\n",
    "    \n",
    "    return X_data\n",
    "\n",
    "X_train = get_images(train_path, output_shape=(img_height,img_width))\n",
    "\n",
    "print(X_train.shape, X_train.dtype)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvJ8CGbtsdzy"
   },
   "source": [
    "#### 1.6 Read and preload the expert annotation masks \n",
    "The following function ``get_labels``  \n",
    "- receives as input a mask path (can be train or test), and a desired image shape for the output.\n",
    "- lists all files in the input path. Hint: use ``os.listdir``.\n",
    "- reads each of the files with ``skimage.io.imread``\n",
    "- uses ``skimage.transform.resize`` to directly downsample the masks according to the ``img_height, img_width`` parameters.\n",
    "- returns a numpy array (matrix) . Hint: define an ``np.array`` of type ``dtype=np.uint8`` to stock the data.\n",
    "- uses ``%%time`` to display the time spent in loading.\n",
    "- Runs the function and print the shape and type of the resulting array (e.g. (670, 256, 256, 3)) uint8 \n",
    "- This can take some minutes (reduce the number of images to accelerate the first tests and reduce memory demands if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXsdGWFGsdzz",
    "outputId": "6f7788de-dda3-4419-c3e3-28004d79fa38"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get training data labels\n",
    "def get_labels(path, output_shape=(None, None)):\n",
    "    '''\n",
    "    Loads and concatenates images from path/{id}/masks/{id}.png into a numpy array\n",
    "    '''\n",
    "    img_paths = [glob.glob('{0}/{1}/masks/*.png'.format(path, id)) for id in os.listdir(path) if id != \".DS_Store\"]\n",
    "    \n",
    "    Y_data = []\n",
    "    for i, img_masks in enumerate(img_paths):  #loop through each individual nuclei for an image and combine them together\n",
    "        masks = skimage.io.imread_collection(img_masks).concatenate()  #masks.shape = (num_masks, img_height, img_width)\n",
    "        mask = np.max(masks, axis=0)                                   #mask.shape = (img_height, img_width)\n",
    "        mask = skimage.transform.resize(mask, output_shape=output_shape+(1,), mode='constant', preserve_range=True)  #need to add an extra dimension so mask.shape = (img_height, img_width, 1)\n",
    "        Y_data.append(mask)\n",
    "\n",
    "    # make sure to return the binary images but coded as floats to be compatible with the loss functions\n",
    "    Y_data = np.array(Y_data, dtype=np.bool)\n",
    "    \n",
    "    return Y_data    \n",
    "    \n",
    "Y_train = get_labels(train_path, output_shape=(img_height,img_width))\n",
    "\n",
    "print(\"We have {} labeled images of size ({},{}) and type {}\".format(Y_train.shape[0],Y_train.shape[1],Y_train.shape[2],Y_train.dtype))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSFj0_ffkTLA"
   },
   "outputs": [],
   "source": [
    "Y_train =1.0*Y_train.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSC-arG7sdz1"
   },
   "source": [
    "#### 1.7 Visualize some examples of images and matching labels\n",
    "Choose one index among the lists of images and labels and use ``plt.imshow`` to visualize side by side the corresponding image and mask.\n",
    "You may want to use ```skimage.exposure.rescale_intensity``` for better visualization of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "yu-ctAxqRx4b",
    "outputId": "677c5645-9198-446a-df42-76dc1ecc0bb1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "idx = np.random.randint(Y_train.shape[0], size=1)[0]\n",
    "\n",
    "\n",
    "print ('Looking at image ', idx)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(skimage.exposure.rescale_intensity(X_train[idx, :, :, :]))\n",
    "plt.title('input image')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Y_train[idx, :, :],cmap='gray')\n",
    "plt.title('mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbTsuLoVsdz4"
   },
   "source": [
    "### 2. Buidling the U-Net model\n",
    "#### 2.1. Define the architecture\n",
    "\n",
    "The u-net is composed of an input layer, an encoder and a decoder. Both the encoder and deconder consists of several blocks with similar structure but different dimensions. Each encoder blocks is composed of two convolutional layers of the same dimensions with a pooling and an activation function. With pooling operations, the 2D dimensions of the image are progressively reduced; to compensate the number of filters (or equivalently of feature channels) in each block is increased. The decoder reverses the operations of the encoder. Finally skip connections are created between the encoder and the decoder. To make these connections  it is useful to create a list pointing to the output of each encoder block as well as a list pointing to the output of each decoder block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvSy-Vq2sdz5"
   },
   "source": [
    "**a)** Building the model\n",
    "- Create a function ```model``` that receives as input the image size \n",
    "- Start by predefining as parameters within the function\n",
    "    - the number of kernels/feature channels per block. Usually they have increasing powers of 2. For instance, ```n_channels=[16,32,64,128,256]```\n",
    "    - a global kernel size ```(3,3)```\n",
    "    - a weight initialization method (try `he_normal`)\n",
    "- Create an input layer using the Keras function ``Input`` , the shape should match the the image dimensions.\n",
    "- Declare the encoder blocks each composed of:\n",
    "    - two convolutional layers. Use ``Conv2D``.\n",
    "    - one maxpool layer (except for the last block). Use ``MaxPooling2D``\n",
    "    - use relu activations\n",
    "    - create a list pointing to each of the encoder blocks to be able to refer to them later while creating the skip connections\n",
    "    - **Hint** Define one block of convolution+pool+relu then create a loop to iterate over the different blocks\n",
    "\n",
    "- Define the decoder blocks each composed of:\n",
    "    - one Transpose convolution layer.\n",
    "    - two convolutional layers. \n",
    "    - use relu activations\n",
    "    - create a list with pointing to each encoder blocks to be able to refer to them later the skip connections\n",
    "    - **Hint** Define one block of transposed convolution+conv+relu then create a loop to iterate over the different blocks\n",
    "\n",
    "\n",
    "- The output layer should be a Transpose convolution layer with a 'sigmoid' activatin and a 'glorot_normal' initialization\n",
    "\n",
    "- Use the lists created above to create the skip connections \n",
    "    - connecting for each resolution the output of the encoder blocks to the input of the decoder blocks.\n",
    "\n",
    "- Use the keras function ``Model`` to identify this function as the architecture and use the output of ```Model``` as the return value of the unet_model function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6cLx3-nsdz6"
   },
   "outputs": [],
   "source": [
    "def unet_model(img_width=256, img_height=256):\n",
    "    \n",
    "    #define the architecture parameters\n",
    "    n_channels = [16,32,64,128,256]  #the number of kernels/feature channels per block SELON TAILLE IMAGE ! \n",
    "    #n_channels = [2**i for i in range(4,int(np.log2(img_width)+1))]\n",
    "    k_size = (3, 3)                  #size of filter kernel\n",
    "    k_init = 'he_normal'             #kernel initializer\n",
    "    encoder_list = []\n",
    "    decoder_list = [] \n",
    "    \n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Use Keras Input layer to create one\n",
    "    inp = Input(shape = (img_width,img_height,3))\n",
    "    \n",
    "    \n",
    "    # Encoder\n",
    "    conv_enc = inp\n",
    "    for i in range(len(n_channels)):                           \n",
    "        conv_1 = tf.keras.layers.Conv2D(n_channels[i], kernel_size = k_size, activation = \"relu\", padding='same')(conv_enc)\n",
    "        conv_2 = tf.keras.layers.Conv2D(n_channels[i], kernel_size = k_size, activation = \"relu\", padding='same')(conv_1)\n",
    "        encoder_list.append(conv_2)\n",
    "        conv_enc = tf.keras.layers.MaxPool2D(pool_size=(2, 2) , strides = 2)(conv_2) \n",
    "        \n",
    "    # Decoder\n",
    "    conv_dec = conv_enc\n",
    "    n_channels.reverse()\n",
    "    for k in range(len(n_channels)):\n",
    "        conv_3 = tf.keras.layers.Conv2DTranspose(n_channels[k],kernel_size = k_size, strides=2 , activation = \"relu\", padding='same')(conv_dec)\n",
    "        decoder_list.append(conv_3)\n",
    "        merge = add([conv_3, encoder_list[-k-1]])\n",
    "        conv_4 = tf.keras.layers.Conv2D(n_channels[k], kernel_size = k_size, activation = \"relu\", padding='same')(merge)\n",
    "        conv_dec = tf.keras.layers.Conv2D(n_channels[k], kernel_size = k_size, activation = \"relu\", padding='same')(conv_4)    \n",
    "        \n",
    "    # Output  \n",
    "    outp = tf.keras.layers.Conv2DTranspose(1,kernel_size = k_size , kernel_initializer='glorot_normal',\n",
    "                                           activation = \"sigmoid\", padding='same')(conv_dec)\n",
    "    \n",
    "    #outp = tf.keras.layers.Dropout(0.2)(outp)\n",
    "    \n",
    "    #Build Model with the architecture\n",
    "    model = Model(inputs=[inp], outputs=[outp])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-ip9xJvsdz8"
   },
   "source": [
    "#### 2.2. Define the losses\n",
    "\n",
    "Define and explain how to use the Dice coefficient and the binarry crossentropy as loss and metrics for the segmentation. Create a costum dice loss that is a weighted average between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRz2dP4_sdz9"
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    score = 2 * (tf.math.reduce_sum(y_true * y_pred) / tf.math.reduce_sum(y_true + y_pred))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coef(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def costum_dice_loss(y_true, y_pred):\n",
    "    loss = 0.5 * dice_loss(y_true, y_pred) + 0.5 * binary_crossentropy(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bJO4Nzjsdz_"
   },
   "source": [
    "#### 2.3 Define the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3bINJ0Tsd0A"
   },
   "outputs": [],
   "source": [
    "# Set optmizer, loss and metric\n",
    "optimizer = \"adam\"\n",
    "loss      = costum_dice_loss\n",
    "metrics   = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi22QZNisd0C"
   },
   "source": [
    "#### 2.4 Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLERRYRnsd0D",
    "outputId": "49d1e796-6291-4597-8383-dba8e299806d"
   },
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "\n",
    "model = unet_model(img_width, img_height)\n",
    "\n",
    "#summary\n",
    "model.summary()\n",
    "#compile the model\n",
    "model.compile(loss = loss , optimizer = optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vJrSrWN_B6_O",
    "outputId": "c163dbd0-ce90-42b1-81ec-0212cd834959"
   },
   "outputs": [],
   "source": [
    "#run the following cell to visualize the created model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vFi09AEsd0F"
   },
   "source": [
    "### 3. Trainning and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UYoVsH6sd0F"
   },
   "source": [
    "#### 3.1 Launch the training\n",
    "- Use the ```model.fit``` function to launc the training, \n",
    "- save the output of the model.fit which provides the history of the training results. \n",
    "- Measure the overall time with ```%%time``` at the beginning of the notebook cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NXX_fehsd0G",
    "outputId": "d6e06eef-3628-4cca-9e15-76148a6ae3cb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit model\n",
    "epochs = 25\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Train accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdFjWK8psd0I"
   },
   "source": [
    "#### 3.2. Save the model\n",
    "Save the final model ``model_out.save_weights`` with an hdf5 format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model1_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Plot the loss and the metrics\n",
    "\n",
    "from the output of the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "DE9rN1ULsd0J",
    "outputId": "44365a34-0e85-4922-bd59-a858f08bd903"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCb0RUyeilpj"
   },
   "source": [
    "#### 3.4. Test\n",
    "Load the test images, make predictions and visualize the predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_images(test_path, output_shape=(img_height,img_width))\n",
    "print(X_test.shape, X_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AN25h6izpJUQ"
   },
   "outputs": [],
   "source": [
    "# if required reload the model\n",
    "model.load_weights('model1_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "plt_counter = 1\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    plt.subplot(5, 4, plt_counter)\n",
    "    plt.imshow(X_test[i])\n",
    "    plt.title(f'Image {i+1}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(5, 4, plt_counter + 1)\n",
    "    plt.imshow(prediction[i], cmap='gray')\n",
    "    plt.title(f'Prédiction {i+1}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt_counter += 2\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Comparison and Improvement\n",
    "\n",
    "Try making changes to improve the results. Do at least one of the following modifications \n",
    "- Change the architecture (use the one from the tf example, or add other types of layers (e.g. dropout) or non-linarities (e.g. leaky relu) and compare the training curves\n",
    "- add data augmentation (rotation, flip, translation of each image)\n",
    "- add an explicit validation set and ilustrate the intermediate results and metrics on the validation set\n",
    "- add data generators (to sample and augment data online)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6cLx3-nsdz6"
   },
   "outputs": [],
   "source": [
    "def unet_model_enhanced(img_width=256, img_height=256):\n",
    "    \n",
    "    #define the architecture parameters\n",
    "    n_channels = [16,32,64,128,256]  #the number of kernels/feature channels per block SELON TAILLE IMAGE ! \n",
    "    #n_channels = [2**i for i in range(4,int(np.log2(img_width)+1))]\n",
    "    k_size = (3, 3)                  #size of filter kernel\n",
    "    k_init = 'he_normal'             #kernel initializer\n",
    "    encoder_list = []\n",
    "    decoder_list = [] \n",
    "    \n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Use Keras Input layer to create one\n",
    "    inp = Input(shape = (img_width,img_height,3))\n",
    "    \n",
    "    # Encoder\n",
    "    conv_enc = inp\n",
    "    for i in range(len(n_channels)):                           \n",
    "        conv_1 = tf.keras.layers.Conv2D(n_channels[i], kernel_size = k_size, activation = \"relu\", padding='same')(conv_enc)\n",
    "        conv_2 = tf.keras.layers.Conv2D(n_channels[i], kernel_size = k_size, activation = \"relu\", padding='same')(conv_1)\n",
    "        encoder_list.append(conv_2)\n",
    "        conv_enc = tf.keras.layers.MaxPool2D(pool_size=(2, 2) , strides = 2)(conv_2) \n",
    "        \n",
    "    # Decoder\n",
    "    conv_dec = conv_enc\n",
    "    n_channels.reverse()\n",
    "    for k in range(len(n_channels)):\n",
    "        conv_3 = tf.keras.layers.Conv2DTranspose(n_channels[k],kernel_size = k_size, strides=2 , activation = \"relu\", padding='same')(conv_dec)\n",
    "        decoder_list.append(conv_3)\n",
    "        merge = add([conv_3, encoder_list[-k-1]])\n",
    "        conv_4 = tf.keras.layers.Conv2D(n_channels[k], kernel_size = k_size, activation = \"relu\", padding='same')(merge)\n",
    "        conv_dec = tf.keras.layers.Conv2D(n_channels[k], kernel_size = k_size, activation = \"relu\", padding='same')(conv_4)    \n",
    "        \n",
    "    # Output  \n",
    "    outp = tf.keras.layers.Conv2DTranspose(1,kernel_size = k_size , kernel_initializer='glorot_normal',\n",
    "                                           activation = \"sigmoid\", padding='same')(conv_dec)\n",
    "    \n",
    "    #outp = tf.keras.layers.Dropout(0.2)(outp)\n",
    "    \n",
    "    #Build Model with the architecture\n",
    "    model = Model(inputs=[inp], outputs=[outp])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLERRYRnsd0D",
    "outputId": "49d1e796-6291-4597-8383-dba8e299806d"
   },
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "\n",
    "model_enhanced = unet_model_enhanced(img_width, img_height)\n",
    "\n",
    "#summary\n",
    "model_enhanced.summary()\n",
    "#compile the model\n",
    "model_enhanced.compile(loss = loss , optimizer = optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NXX_fehsd0G",
    "outputId": "d6e06eef-3628-4cca-9e15-76148a6ae3cb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit model\n",
    "epochs = 32\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "history_enhanced = model_enhanced.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_enhanced.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Train accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "DE9rN1ULsd0J",
    "outputId": "44365a34-0e85-4922-bd59-a858f08bd903",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_enhanced.history['accuracy'], label='Training')\n",
    "plt.plot(history_enhanced.history['val_accuracy'], label='Validation')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_enhanced.history['val_loss'], label='Validation')\n",
    "plt.plot(history_enhanced.history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "\n",
    "prediction = model_enhanced.predict(X_test)\n",
    "\n",
    "plt_counter = 1\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    plt.subplot(5, 4, plt_counter)\n",
    "    plt.imshow(X_test[i])\n",
    "    plt.title(f'Image {i+1}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(5, 4, plt_counter + 1)\n",
    "    plt.imshow(prediction[i], cmap='gray')\n",
    "    plt.title(f'Prédiction {i+1}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt_counter += 2\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SOLUTION 2020-2021-Semantic segmentation Changing to TF2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
