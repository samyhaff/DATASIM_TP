{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 01 - Feature Selection and Dimentionality Reduction\n",
    "** Ecole Centrale Nantes **\n",
    "\n",
    "** Diana Mateus **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTICIPANTS: Yassine JAMOUD, Samy Haffoudhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear Regression\n",
    "\n",
    "**a) Run the code to create a toy dataset**. The dataset has 100 points each described by three features $x_1$, $x_2$ and $x_3$. The target value $y$ is continuous and is linearly generated from the three variables (and noise). Notice how the three variables are created.\n",
    "\n",
    "**b)** **Training a linear regression model** Use the scikit learn in-built functions for fiting a linear model to the created dataset \n",
    "\n",
    "``model = LinearRegression()``\n",
    "\n",
    "``model.fit(X,Y)``\n",
    "\n",
    "The goal is to automatically estimate the parameters $w_1$, $w_2$ and $w_3$ of the linear model from the input datamatrix $\\mathbb{X}$ and the target values ${y}$. \n",
    "\n",
    "Look at the documentation of the ``LinearRegression`` function to recover the estimated values of the intercept  (ordonnée à l'origine) $b$ and coefficient $w_1$, $w_2$, $w_3$ parameters.\n",
    "\n",
    "**BONUS** Alternatively use the Ordinary Least Squares analytical solution to recover the model parameters\n",
    "\n",
    "**c)** **Evaluating the target predictions** \n",
    "Use the ``model.predict`` function to estimate the predictions $y_{hat}$ for the training dataset $X$.\n",
    "\n",
    "- Plot $y$ vs. $y_{hat}$\n",
    "- Compute the mean squared error and the r2 variance error between the estimated and the ground truth outputs. \n",
    "- Describe your findings\n",
    "\n",
    "**d)** **Comparing the ground truth vs the estimated model parameters** \n",
    "- Compare the values of the original parameters to the estimated ones. Are the estimated values for the coefficients $w_1$, $w_2$, $w_3$ and the intercept $b$ close to the original model?\n",
    "- What can you say about the contribution of each input feature to the output?\n",
    "- Explain why do we get such results? \n",
    "- How can we improve the interpretability of the weights? at what cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Create dataset \n",
    "#Check another way to create correlated data at the end of the notebook\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "size = 100\n",
    "np.random.seed(seed=5)\n",
    " \n",
    "X_seed = np.random.normal(0, 1, size) \n",
    "X1 = X_seed + np.random.normal(0, .1, size)\n",
    "X2 = X_seed + np.random.normal(0, .01, size)\n",
    "X3 = X_seed + np.random.normal(0, .001, size)\n",
    "X = np.array([X1, X2, X3]).T\n",
    "\n",
    "intercept = 10+np.random.normal(0,1, size).reshape((size,1))\n",
    "\n",
    "W = np.ones((3,1))\n",
    "\n",
    "#Y = X1 + X2 + X3 + noisy10\n",
    "Y = np.matmul(X,W)+ intercept\n",
    "\n",
    "print('Shape of X',np.shape(X))\n",
    "print('Shape of W', np.shape(W))\n",
    "print('Shape of Y', np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Fitting a Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "estimated_coeffs = model.coef_\n",
    "estimated_intercept = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Plot y against y*\n",
    "\n",
    "Yhat = model.predict(X)\n",
    "\n",
    "plt.plot(Y, Yhat, '.')\n",
    "plt.plot([0, 18], [0, 18])\n",
    "plt.title('y against y*')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y*')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"% mean_squared_error(Yhat, Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(Yhat, Y))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'on obtient une valeur de r2 assez proche de la valeur 1.0 et une erreur MSE de 0.89. Les prédictions sont donc satisfaisantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original: w1 = {W[0, 0]}, w2 = {W[1, 0]}, w3 = {W[2, 0]}\")\n",
    "print(f\"Estimated: w1 = {estimated_coeffs[0, 0]}, w2 = {estimated_coeffs[0, 1]}, w3 = {estimated_coeffs[0, 2]}\")\n",
    "print(f'Estimated: intercept = {estimated_intercept[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On remarque que les coefficients obtenus sont eloignés des coefficients originaux. \n",
    "* Les valeurs obtenues permettent d'ordonner la contribution de chaque feature à la prédiction par ordre décroissant : X2, X3, X1.\n",
    "* Ces résultats s'expliquent par l'instabilité de la solution obtenue par moindres carrés et qui amplifie le bruit sur l'ordonnée à l'origine.\n",
    "* On peut y remédier en introduisant un terme de régularisation qui va permettre de pénaliser les valeurs élevées des coefficients. On réduire alors la variance mais on augmentera le bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ridge and Lasso Regression\n",
    "\n",
    "a) Using the data from above repeat the regression but fitting this time:\n",
    "- a ridge model ```Ridge()```\n",
    "- a Lasso model ```Lasso()```\n",
    "- an Elastic Net model ```ElasticNet()```\n",
    "\n",
    "b) Compare the **prediction errors** (MSE, r2), among the three regularized models.  Use a fixed value of the regularization coefficient ```alpha=0.3``` (alpha corresponds to $\\lambda$ in the lectures). Comment on the results.\n",
    "\n",
    "c) Plot the estimated coefficients against different values of ``alpha``($\\lambda$). Use the following values \n",
    "``` python\n",
    "alphas = np.logspace(-6, 2, 200)\n",
    "```\n",
    "d) **Coefficient estimate error** Mesure the MSE error between the original and estimated parameters for each case. Plot the coefficient error vs alpha.\n",
    "\n",
    "e) **Prediction error** Mesure the MSE error between the ground truth and predicted target for each case. Plot the target error vs alpha.\n",
    "\n",
    "f) Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Ridge Lasso ElasticNet\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "fig=plt.figure(figsize=(18, 5))\n",
    "\n",
    "alpha = 0.3\n",
    "\n",
    "model = Ridge(alpha)\n",
    "model.fit(X, Y)\n",
    "\n",
    "Yhat_ridge = model.predict(X)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(Y, Yhat_ridge, '.')\n",
    "plt.plot([0, 18], [0, 18])\n",
    "plt.title('Ridge')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y*')\n",
    "\n",
    "model = Lasso(alpha)\n",
    "model.fit(X, Y)\n",
    "\n",
    "Yhat_lasso = model.predict(X)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(Y, Yhat_lasso, '.')\n",
    "plt.plot([0, 18], [0, 18])\n",
    "plt.title('Lasso')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y*')\n",
    "\n",
    "model = ElasticNet(alpha)\n",
    "model.fit(X, Y)\n",
    "\n",
    "Yhat_enet = model.predict(X)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(Y, Yhat_enet, '.')\n",
    "plt.plot([0, 18], [0, 18])\n",
    "plt.title('Elastic Net')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y*')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Errors\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"% mean_squared_error(Yhat, Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f\\n' % r2_score(Yhat, Y))\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error ridge: %.2f\"% mean_squared_error(Yhat_ridge, Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score ridge: %.2f\\n' % r2_score(Yhat_ridge, Y))\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error lasso: %.2f\"% mean_squared_error(Yhat_lasso, Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score lasso: %.2f\\n' % r2_score(Yhat_lasso, Y))\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error enet: %.2f\"% mean_squared_error(Yhat_enet, Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score enet: %.2f\\n' % r2_score(Yhat_enet, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe, comme dit plus haut, que le biais a augmenté avec l'introduction du terme de régularisation pour ces 3 nouvelles méthodes par rapport à la première. Les résultats sur les données d'entrainement sont inférieurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c-d-e)Plot the estimated coefficients and errors against different values of alpha\n",
    "\n",
    "fig=plt.figure(figsize=(18, 8))\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "alphas = np.logspace(-6, 2, 200)\n",
    "w1_values = w2_values = w3_values = np.zeros((200,3))\n",
    "intercept_values = np.zeros((200,3))\n",
    "prediction_errors = np.zeros((200,3))\n",
    "\n",
    "W_estimad = np.array([], dtype='float64')\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Ridge(alpha)\n",
    "    model.fit(X, Y)\n",
    "    w1_ridge, w2_ridge, w3_ridge = model.coef_[0]\n",
    "    intercept_ridge = model.intercept_\n",
    "    Yhat_ridge = model.predict(X)\n",
    "    prediction_errors_ridge = mean_squared_error(Yhat_ridge, Y)\n",
    "\n",
    "    model = Lasso(alpha)\n",
    "    model.fit(X, Y)\n",
    "    w1_lasso, w2_lasso, w3_lasso = model.coef_\n",
    "    intercept_lasso = model.intercept_\n",
    "    Yhat_lasso = model.predict(X)\n",
    "    prediction_errors_lasso = mean_squared_error(Yhat_lasso, Y)\n",
    "\n",
    "    model = ElasticNet(alpha)\n",
    "    model.fit(X, Y)\n",
    "    w1_enet, w2_enet, w3_enet = model.coef_\n",
    "    intercept_enet = model.intercept_\n",
    "    Yhat_enet = model.predict(X)\n",
    "    prediction_errors_enet = mean_squared_error(Yhat_enet, Y)\n",
    "    \n",
    "    w1_values[i, :] = [w1_ridge, w1_lasso, w1_enet]\n",
    "    w2_values[i, :] = [w2_ridge, w2_lasso, w2_enet]\n",
    "    w3_values[i, :] = [w3_ridge, w3_lasso, w3_enet]\n",
    "    \n",
    "    intercept_values[i, :] = [intercept_ridge, intercept_lasso, intercept_enet]\n",
    "    \n",
    "    prediction_errors[i, :] = [prediction_errors_ridge, prediction_errors_lasso, prediction_errors_enet]\n",
    "    \n",
    "coef_errors = 1/4 * (w1_values-1)**2 + (w2_values-1)**2 + (w3_values-1)**2 + (intercept_values-10)** 2\n",
    "    \n",
    "plt.subplot(2,4,1)\n",
    "plt.plot(alphas, w1_values[:, 0])\n",
    "plt.plot(alphas, w1_values[:, 1])\n",
    "plt.plot(alphas, w1_values[:, 2])\n",
    "plt.legend([\"Ridge\", \"Lasso\", \"Enet\"])\n",
    "plt.title('w1')\n",
    "plt.subplot(2,4,2)\n",
    "plt.plot(alphas, w2_values[:, 0])\n",
    "plt.plot(alphas, w2_values[:, 1])\n",
    "plt.plot(alphas, w2_values[:, 2])\n",
    "plt.legend([\"Ridge\", \"Lasso\", \"Enet\"])\n",
    "plt.title('w2')\n",
    "plt.subplot(2,4,3)\n",
    "plt.plot(alphas, w3_values[:, 0])\n",
    "plt.plot(alphas, w3_values[:, 1])\n",
    "plt.plot(alphas, w3_values[:, 2])\n",
    "plt.legend([\"Ridge\", \"Lasso\", \"Enet\"])\n",
    "plt.title('w3')\n",
    "plt.subplot(2,4,4)\n",
    "plt.plot(alphas, intercept_values[:, 0])\n",
    "plt.plot(alphas, intercept_values[:, 1])\n",
    "plt.plot(alphas, intercept_values[:, 2])\n",
    "plt.legend([\"Ridge\", \"Lasso\", \"Enet\"])\n",
    "plt.title('intercept')\n",
    "plt.subplot(2,4,5)\n",
    "plt.plot(alphas, coef_errors[:, 0])\n",
    "plt.plot(alphas, coef_errors[:, 1])\n",
    "plt.plot(alphas, coef_errors[:, 2])\n",
    "plt.legend([\"Ridge\", \"Lasso\", \"Enet\"])\n",
    "plt.title('Coef MSE')\n",
    "plt.subplot(2,4,6)\n",
    "plt.plot(alphas, prediction_errors[:, 0])\n",
    "plt.plot(alphas, prediction_errors[:, 1])\n",
    "plt.plot(alphas, prediction_errors[:, 2])\n",
    "plt.legend([\"Ridge\", \"Lasso\", \"Enet\"])\n",
    "plt.title('Prediction MSE')\n",
    "\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, on observe à l'aide du graphique représentant la MSE sur les coefficients, qu'un bon choix du paramètre alpha permet de retrouver des valeurs de coefficients correspondant de celles attendues pour chacune des méthodes. Par exemple les méthodes Laso et Enet nécessitent une valeur de alpha bien plus faible que celle pour la méthode ridge pour minimiser les MSE sur les coefficients et l'erreur de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature selection  for Heart Disease\n",
    "The dataset ```filtHeartDataSet``` is a filtered version and subset of Heart dataset which contains a binary outcome labels for 299 patients  with chest pain. \n",
    "- A positive target value indicates the presence of heart disease based on an angiographic test, \n",
    "- while a negative target value means no heart disease. \n",
    "\n",
    "The data matrix dataMatrix contains 13 features (measurements) including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.\n",
    "\n",
    "See the following link for the full description\n",
    "http://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "**Goal**: Find and retain only the most relevant features to predict heart disease\n",
    "\n",
    "**a)** Run the given code to load and prepare the Heart Disease dataset.\n",
    "\n",
    "**b)** Compute the correlation of the target to every value and between variables **Hint** use ``np.corr`` on the matrix containing both the target values and the variables.\n",
    "\n",
    "**c)** Run the skitlearn example for univariate feature selection with  as criteria.\n",
    "\n",
    "**d)** Modify the example to do feature selection on the Heart Disease dataset. Try the ``f-test`` and ``mutual information`` univariate tests for classification. \n",
    "\n",
    "**e)** Compare the results of d) against those of Lasso and Elastic Net regression.\n",
    "\n",
    "**f)**  What are the most predictive variables to be preserved? Recover the actual name of the variables in each case.How many variables should we keep?\n",
    "\n",
    "**g)** What type of variable selection methods are the univariate test and Lasso and Elastic Net? Which type of method is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Load and prepare data\n",
    "#The variables of interest are Xall and yall\n",
    "#The final ordered list of variable names is found in new_columns_2\n",
    "\n",
    "import pandas as pd\n",
    "# Create Pandas dataframe.\n",
    "columns = [\"age\", \"sex\", \"cp\", \"restbp\", \"chol\", \"fbs\", \"restecg\", \n",
    "           \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n",
    "df0     = pd.read_table(\"heart_disease_all14.csv\", sep=',', header=None, names=columns)\n",
    "# Convert categorical variables with more than two values into dummy variables.\n",
    "# Note that variable ca is discrete but not categorical, so we don't convert it.\n",
    "df      = df0.copy()\n",
    "dummies = pd.get_dummies(df[\"cp\"],prefix=\"cp\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"cp\"]\n",
    "del df[\"cp_4.0\"]\n",
    "df      = df.rename(columns = {\"cp_1.0\":\"cp_1\",\"cp_2.0\":\"cp_2\",\"cp_3.0\":\"cp_3\"})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"restecg\"],prefix=\"recg\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"restecg\"]\n",
    "del df[\"recg_0.0\"]\n",
    "df      = df.rename(columns = {\"recg_1.0\":\"recg_1\",\"recg_2.0\":\"recg_2\"})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"slope\"],prefix=\"slope\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"slope\"]\n",
    "del df[\"slope_2.0\"]\n",
    "df      = df.rename(columns = {\"slope_1.0\":\"slope_1\",\"slope_3.0\":\"slope_3\"})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"thal\"],prefix=\"thal\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"thal\"]\n",
    "del df[\"thal_3.0\"]\n",
    "df      = df.rename(columns = {\"thal_6.0\":\"thal_6\",\"thal_7.0\":\"thal_7\"})\n",
    "\n",
    "# Replace response variable values and rename\n",
    "df[\"num\"].replace(to_replace=[1,2,3,4],value=1,inplace=True)\n",
    "df      = df.rename(columns = {\"num\":\"hd\"})\n",
    "\n",
    "# New list of column labels after the above operations\n",
    "new_columns_1 = [\"age\", \"sex\", \"restbp\", \"chol\", \"fbs\", \"thalach\", \n",
    "                 \"exang\", \"oldpeak\", \"ca\", \"hd\", \"cp_1\", \"cp_2\",\n",
    "                 \"cp_3\", \"recg_1\", \"recg_2\", \"slope_1\", \"slope_3\",\n",
    "                 \"thal_6\", \"thal_7\"]\n",
    "\n",
    "print ('\\nNumber of patients in dataframe: %i, with disease: %i, without disease: %i\\n' \\\n",
    "      % (len(df.index),len(df[df.hd==1].index),len(df[df.hd==0].index)))\n",
    "\n",
    "#print (df.head()) # UNCOMMENT FOR MORE INFO ON THE DATASET\n",
    "#print (df.describe())# UNCOMMENT FOR MORE INFO ON THE DATASET\n",
    "\n",
    "# Standardize the dataframe\n",
    "stdcols = [\"age\",\"restbp\",\"chol\",\"thalach\",\"oldpeak\"]\n",
    "nrmcols = [\"ca\"]\n",
    "stddf   = df.copy()\n",
    "stddf[stdcols] = stddf[stdcols].apply(lambda x: (x-x.mean())/x.std())\n",
    "stddf[nrmcols] = stddf[nrmcols].apply(lambda x: (x-x.mean())/(x.max()-x.min()))\n",
    "\n",
    "new_columns_2 = new_columns_1[:9] + new_columns_1[10:]\n",
    "new_columns_2.insert(0,new_columns_1[9])\n",
    "stddf = stddf.reindex(columns=new_columns_2)\n",
    "\n",
    "# Convert dataframe into numpy arrays to be used by classifiers\n",
    "yall = stddf[\"hd\"] # the heart disease column alone\n",
    "Xall = stddf[new_columns_2[1:]].values # the potentially predictive variables\n",
    "yXall = stddf.values # y and X combined in the same matrix, with y in the first column\n",
    "\n",
    "print(\"Number of available features:\", Xall.shape[1])\n",
    "stddf[new_columns_2[1:]].head() #only the variables\n",
    "\n",
    "print(new_columns_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Correlation matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)  Scikit learn example for monovariate feature selection on iris dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "\n",
    "iris = load_iris()\n",
    "Xiris, yiris = iris.data, iris.target\n",
    "print('Shape of the input matrix', X.shape)\n",
    "\n",
    "model = SelectKBest(f_classif, k=2)\n",
    "model.fit(Xiris,yiris)\n",
    "mask = model.get_support()\n",
    "print('Selected best variables',mask)\n",
    "\n",
    "Xiris_new=Xiris[:,mask==True]\n",
    "print('Shape after variable selection',Xiris_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Run univariate feature selection on the heart disease dataset\n",
    "\n",
    "model = SelectKBest(f_classif, k=2)\n",
    "model.fit(Xall, yall)\n",
    "mask = model.get_support()\n",
    "# Xall_f = Xall[:,mask==True]\n",
    "print(\"Selected best variables\", mask)\n",
    "\n",
    "model = SelectKBest(mutual_info_classif, k=2)\n",
    "model.fit(Xall, yall)\n",
    "mask = model.get_support()\n",
    "# Xall_mutual_info = Xall[:,mask==True]\n",
    "print(\"Selected best variables\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Lasso and Enet \n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "alphas = np.logspace(-4, 0, 200)\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = SelectFromModel(Lasso(alpha), threshold='mean')\n",
    "    model.fit(Xall, yall)\n",
    "    mask = model.get_support()\n",
    "    print(\"alpha = \", alpha)\n",
    "    print(\"Lasso Selected best variables\", mask)\n",
    "    \n",
    "    model = SelectFromModel(ElasticNet(alpha), threshold='mean')\n",
    "    model.fit(Xall, yall)\n",
    "    mask = model.get_support()\n",
    "    print(\"Enet Selected best variables\", mask, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) \n",
    "\n",
    "fig=plt.figure(figsize=(10, 5))\n",
    "\n",
    "model = SelectKBest(f_classif, k=\"all\")\n",
    "model.fit(Xall, yall)\n",
    "\n",
    "_, n_features = Xall.shape\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(range(1, n_features+1), model.scores_)\n",
    "plt.title('f')\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "model = SelectKBest(mutual_info_classif, k=\"all\")\n",
    "model.fit(Xall, yall)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(range(1, n_features+1), model.scores_)\n",
    "plt.title('mutual_info')\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "def getName(l, mask):\n",
    "    return [l[i] for i in range(len(l)) if mask[i]]\n",
    "\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut s'aider des deux graphiques ci-dessus pour sélectionner des features en conservant uniquement les k au score le plus élevé. On choisit alors d'éliminer les 6 features avec le score f le plus faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectKBest(f_classif, k=n_features-6)\n",
    "model.fit(Xall, yall)\n",
    "mask = model.get_support()\n",
    "\n",
    "getName(new_columns_2[1:], mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La méthode \"univariate test\" correspond à une approche par filtrage\n",
    "* Les méthodes reposant sur la régression Lasso et Elastic Net sont correspondent à des \"embedded approaches\"\n",
    "* Il reste alors à tester des méthodes relevant d'une \"wrapper approach\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. PCA demo with point cloud\n",
    "Run and analyse the demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Generating the data\n",
    "np.random.seed(1)\n",
    "X = np.matmul(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
    "plt.plot(X[:, 0], X[:, 1], 'o')\n",
    "plt.axis('equal');\n",
    "\n",
    "#Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print('Explained variance', pca.explained_variance_)\n",
    "print('PCA components\\n',pca.components_)\n",
    "\n",
    "#Plot principal components\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
    "plt.axis('equal');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Own PCA implementation \n",
    "Make your own implementation of the PCA algorithm and compare your results with the above\n",
    "\n",
    "To implement PCA follow the steps bellow\n",
    "- Demean the data (mean=0).\n",
    "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix (Equivalently do Singular Vector Decomposition on the data Matrix). \n",
    "``` np.linalg.eig(cov_mat)```\n",
    "- Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues. Remember k is the number of dimensions of the new feature subspace (k ≤ D) (Check first if already ordered)\n",
    "- Construct the projection matrix T from the selected k eigenvectors.\n",
    "- Transform the original dataset X via T to obtain a k dimensional feature subspace Z\n",
    "\n",
    "Your results should be equivalent to the above, explain any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, k):\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "    C = np.cov(X_centered, rowvar=False)\n",
    "    w, v = np.linalg.eigh(C)\n",
    "    sort_index = np.argsort(w[::-1])\n",
    "    w_sorted, v_sorted = w[sort_index], v[:, sort_index]\n",
    "    v_subset, w_subset = v_sorted[:,:k], w_sorted[:k]\n",
    "    X_reduced = X_centered @ v_subset.T\n",
    "        \n",
    "    return (X_reduced, v_subset.T, w_subset)\n",
    "\n",
    "_, components, explained_variance = pca(X, 2)\n",
    "\n",
    "print(explained_variance)\n",
    "print(components)\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "for length, vector in zip(explained_variance, components):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
    "plt.axis('equal');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que le sens du vecteur de la 2e composante principale est opposé par rapport à celui obtenu avec l'implémentation de scikitlearn, mais seule la direction nous intéresse et cette différence s'explique par un choix différent de vecteur propre associé à cette 2e composante principale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 PCA on digits dataset\n",
    "\n",
    "Apply PCA (yours or the in-built version) on the load_digits dataset\n",
    "\n",
    "**a)** Visualize some of the images for each target value.\n",
    "\n",
    "**b)** Apply PCA to reduce the dimensionality of each vectorized image (1,64) to just 2 dimensions. Plot the projected dataset with a scatter plot in two dimensions, using the labels to color. Comment.\n",
    "\n",
    "Hint for plotting:\n",
    "``` python\n",
    "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', \n",
    "            alpha=0.5,cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar();\n",
    "```\n",
    "\n",
    "**c)** Visualize the cumulative explained variance vs the the number of retained dimensions\n",
    "\n",
    "**d)** For 3 different input images (from different target values) \n",
    "- recostruct and show the full 8x8 image from its 1x2 low-dimensional representation.\n",
    "Hint: Use ```pca.inverse_transform```, \n",
    "\n",
    "- show how the aspect of the reconstructed image changes when increasing the number of retained dimensions.\n",
    "\n",
    "**e)** what does the inverse_transformation function do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Load and visualize data\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print('Original size',X.shape)\n",
    "\n",
    "# a) Show some of the images\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    r = np.random.randint(1,X.shape[0])\n",
    "    im = X[r,:]\n",
    "    #print(im.shape)\n",
    "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "    ax.text(0.95, 0.05, 'n = {0}'.format(r), ha='right',\n",
    "            transform=ax.transAxes, color='green')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) 2D projection of the images\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# print('Explained variance', pca.explained_variance_)\n",
    "# print('PCA components\\n',pca.components_)\n",
    "\n",
    "Xproj = pca.transform(X)\n",
    "\n",
    "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', \n",
    "            alpha=0.5,cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar();\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Cummulative explained variance\n",
    "\n",
    "M, N = X.shape\n",
    "cumulative_explained_variances = []\n",
    "\n",
    "for d in range(1, N):\n",
    "    pca = PCA(n_components=d)\n",
    "    pca.fit(X)\n",
    "    cumulative_explained_variances.append(np.sum(pca.explained_variance_))\n",
    "    \n",
    "plt.plot(range(1, N), cumulative_explained_variances)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.title(\"Cumulative explained variance vs number of components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d) Inverse transform for reconstruction (from low to high dimensional representation)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 5))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "Xproj = pca.transform(X)\n",
    "\n",
    "Xorig = pca.inverse_transform(Xproj)\n",
    "\n",
    "plt.subplot(3,3,1)\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "plt.imshow(X[0,:].reshape((8,8)), cmap='binary')\n",
    "plt.subplot(3,3,2)\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "plt.imshow(X[1,:].reshape((8,8)), cmap='binary')\n",
    "plt.subplot(3,3,3)\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "plt.imshow(X[2,:].reshape((8,8)), cmap='binary')\n",
    "plt.subplot(3,3,4)\n",
    "plt.axis('off')\n",
    "plt.title('Reconstructed')\n",
    "plt.imshow(Xorig[0,:].reshape((8,8)), cmap='binary')\n",
    "plt.subplot(3,3,5)\n",
    "plt.axis('off')\n",
    "plt.title('Reconstructed')\n",
    "plt.imshow(Xorig[1,:].reshape((8,8)), cmap='binary')\n",
    "plt.subplot(3,3,6)\n",
    "plt.axis('off')\n",
    "plt.title('Reconstructed')\n",
    "plt.imshow(Xorig[2,:].reshape((8,8)), cmap='binary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `inverse_transform(X)` retourne, comme son nom l'indique, la valeur dont le calcul de la PCA donnerait X. Comme on le voit ci-dessus on obtient pas nécessairement la reconstruction exacte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "\n",
    "for d in range(1, 64):\n",
    "    pca = PCA(n_components=d)\n",
    "    pca.fit(X)\n",
    "    Xproj = pca.transform(X)\n",
    "    Xorig = pca.inverse_transform(Xproj)\n",
    "    err.append(sum((X[0,:]-Xorig[0,:])**2))\n",
    "\n",
    "plt.plot(range(1,64), err)\n",
    "plt.title(\"MSE image 1 reconstruction\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe alors qu'en augmentant le nombre de composantes, on obtient une meilleure approximation de l'image. Par exemple pour l'image 1, on obtient l'image originale pour environ 38 composantes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
