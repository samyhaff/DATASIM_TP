{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-Stumps, trees and forests\n",
    "\n",
    "** Ecole Centrale Nantes **\n",
    "\n",
    "** Diana Mateus **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTICIPANTS: **Yassine Jamoud and Samy Haffoudhi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision stumps\n",
    "A decision stump is a machine learning model consisting of a one-level decision tree. That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules [Wikipedia]\n",
    "\n",
    "***a)*** Run the provided code to generate and plot a toy dataset consisting of 2D points and 4 classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Load and plot dataset, split in train and test sets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_classes = 4\n",
    "X, y = make_blobs(n_samples=300, centers=n_classes,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.get_cmap('rainbow', 4));\n",
    "plt.colorbar();\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**b)** Observe the functions **stump** and **split**. \n",
    "\n",
    "**stump** is a function that generates the parameters of a random axis-aligned split  according to the number of features of a given dataset. The function receives the data matrix Xtrain and returns\n",
    "    - the index of one randomly chosen feature (one dimension) \n",
    "    - as well as a randomly chosen threshold within the min and max values of the chosen feature.\n",
    "\n",
    "**split**  receives as input:\n",
    "    - A dataset of points\n",
    "    - the parameters generated by the stump function above. \n",
    "The function then partitions the *dataset* in two subsets according to the threshold of the chosen dimension.\n",
    "The output are two arrays, each containing the _indices_ of the points belonging to one or the other subset. \n",
    "\n",
    "**c)** Run the split function **several times**, and display the resulting subsets as 2D scatter plots with circles of different colors for each class. Use the provided plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint, uniform\n",
    "\n",
    "# b) Stumps and Split\n",
    "\n",
    "def stump(X):\n",
    "    f = randint(0, X.shape[1]-1) #randomly select a feature/dimension randint()\n",
    "    t = uniform(np.min(X[:,f]),np.max(X[:,f])) #sample from an uniform() between the min and max values of the feature\n",
    "    return f,t \n",
    "\n",
    "\n",
    "def split(X, f, t):\n",
    "    ind_l, = np.where(X[:,f]<=t)\n",
    "    ind_r, = np.where(X[:,f]>t)\n",
    "    return ind_l, ind_r\n",
    "\n",
    "# c) Plotting a stump\n",
    "\n",
    "def plot_stump (Xtrain,ytrain,feat,th,ind_l,ind_r):\n",
    "    n_classes=np.max(ytrain)+1;\n",
    "    #print(n_classes)\n",
    "    for c in range(n_classes):\n",
    "        X0_c=Xtrain[np.where(ytrain==c), 0] #first coordinate of points belonging to class c\n",
    "        X1_c=Xtrain[np.where(ytrain==c), 1] #second coordinate of points belonging to class c\n",
    "        plt.scatter(X0_c,X1_c, s=50,alpha=0.5,cmap=plt.cm.get_cmap('rainbow', 4),label=str(c))\n",
    "\n",
    "    #plt.axis('equal')\n",
    "\n",
    "    #draw the points on the left and right child as circles around the original training dataset\n",
    "    plt.scatter(Xtrain[ind_l, 0], Xtrain[ind_l, 1], c='none', edgecolor='r')\n",
    "    plt.scatter(Xtrain[ind_r, 0], Xtrain[ind_r, 1], c='none', edgecolor='b')\n",
    "\n",
    "    #draw threshold line\n",
    "    if feat == 0:\n",
    "        plt.plot([th,th],[np.min(Xtrain[:,1]),np.max(Xtrain[:,1])])\n",
    "    elif feat == 1:\n",
    "        plt.plot([np.min(Xtrain[:,0]),np.max(Xtrain[:,0])],[th,th])\n",
    "    leg = plt.legend();\n",
    "\n",
    "n_stumps = 5\n",
    "for _ in range(n_stumps):\n",
    "    feat, th = stump(Xtrain)\n",
    "    print('Stump parameters', feat, th)\n",
    "    ind_l,ind_r = split(Xtrain, feat, th)\n",
    "    print('Sizes: original set:', len(ytrain), \n",
    "          ' left subset:' ,  ind_l.shape,\n",
    "          ' right subset:', ind_r.shape)\n",
    "\n",
    "plot_stump(Xtrain,ytrain,feat,th,ind_l,ind_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Implement a function ***class_distributions*** that given a set of points reaching a node returns the posterior class distribution of the node (approximated as the normalized histogram with #points per class).\n",
    "\n",
    "``` python\n",
    "def class_distributions(ytrain,ind_left, ind_right):\n",
    "```\n",
    "\n",
    "Compute the class distributions of:\n",
    "    - of the original training set (before the split), \n",
    "    - of each of the 2 subsets resulting from after the split has been applied.\n",
    "\n",
    "Plot the histograms of the children nodes\n",
    "\n",
    "**e)** Create a function to compute the ***information_gain*** of a split. The function should receive the full training dataset (Xtrain and ytrain) and the indices of two subsets resulting from the current split. \n",
    "\n",
    "``` python\n",
    "def information_gain(ytrain,ind_left, ind_right):\n",
    "```\n",
    "```Hint: ```  when computing the Entropy, ignore the classes with zero probabilities, and carry on summation using the same equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distributions(ytrain, ind_left, ind_right):\n",
    "    left_distribution=[len(np.where(ytrain[ind_left]==i)[0]) for i in range(4)]\n",
    "    right_distribution=[len(np.where(ytrain[ind_right]==i)[0]) for i in range(4)]\n",
    "    return left_distribution, right_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_distribution, right_distribution = class_distributions(ytrain,ind_l,ind_r)\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(4), left_distribution)\n",
    "plt.xlabel('Class')\n",
    "plt.title('Left Distribution')\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(4), right_distribution)\n",
    "plt.xlabel('Class')\n",
    "plt.title('Right Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    return -sum([(y == i).sum() / len(y) * np.log2((y == i).sum() / len(y)) for i in range(np.max(ytrain)+1) if (y == i).sum() > 0])\n",
    "\n",
    "def information_gain(ytrain, ind_left, ind_right):\n",
    "    return entropy(ytrain) - (len(ind_left) / len(ytrain)) * entropy(ytrain[ind_left]) - (len(ind_right) / len(ytrain)) * entropy(ytrain[ind_right]) \n",
    "               \n",
    "# information_gain(ytrain, ind_l, ind_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Create a function ***train*** to perfom a *randomized node optimization*. The function receives as parameters the full training dataset (Xtrain and ytrain) as well as the number of random stumps to try. The function will \n",
    "- generate the desired number of stumps, \n",
    "- split the dataset according to each stump, \n",
    "- evaluate the ***information gain*** for each split\n",
    "- choose and then return the parameters of the best stump.\n",
    "\n",
    "``` python\n",
    "def train_stumps(Xtrain, ytrain, trials):\n",
    "```\n",
    "\n",
    "Print the progress of the information gain during the training process\n",
    "\n",
    "**g)** Make predictions with the trained model and display the restuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stumps(Xtrain, ytrain, trials):\n",
    "    best_information_gain = float('-inf')\n",
    "    for _ in range(trials):\n",
    "        f, t = stump(Xtrain)\n",
    "        ind_l, ind_r = split(Xtrain, f, t)\n",
    "\n",
    "        curr = information_gain(ytrain, ind_l, ind_r)\n",
    "        if curr > best_information_gain:\n",
    "            best_information_gain = curr\n",
    "            f_best, t_best = f, t\n",
    "            \n",
    "    return f_best, t_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t = train_stumps(Xtrain, ytrain, 100)\n",
    "ind_l, ind_r = split(Xtrain, f, t)\n",
    "plot_stump (Xtrain,ytrain,f,t,ind_l,ind_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing classifiers on the Caltech 101 dataset\n",
    "\n",
    "Compare the performances of an SVM, a Decision Tree and Random Forest classifiers on the **Caltech 101** dataset for a binary classification task (e.g. dragonfly vs crayfish)\n",
    "\n",
    "\n",
    "**a)** Load and split the dataset into train and test with 75% and 25% respective proportions\n",
    "\n",
    "**b)** Train three classification models on the train dataset \n",
    "- an SVM\n",
    "- a single decision tree ``sklearn.tree.DecisionTree``\n",
    "- a Random Forest ``sklearn.ensemble.RandomForestClassifier``\n",
    "\n",
    "Print the accuracy on the train and test sets for each model. ``from sklearn import metrics``\n",
    "\n",
    "**c)** Plot the roc curves for each model, and compute the areas under the curve\n",
    "\n",
    "**d)** Do a gridsearch with a 5-fold crossvalidation varying the hyperparameters of each model (e.g., for the SVM the ``kernel``, the ``gamma`` and ``c``, and for the tree-based models  ``max_depth``, ``max_features``, ``min_samples_leaf``, and ``n_estimators``).\n",
    "\n",
    "``from sklearn.model_selection import GridSearchCV``\n",
    "\n",
    "**Hint**\n",
    "```\n",
    "tuned_parameters = [{'max_depth': [1, 5, 10], \n",
    "                     'max_features': [1, 15, 30, 45, 60],\n",
    "                     'n_estimators': [1, 25, 50, 75, 100],\n",
    "                     'min_samples_leaf': [3,5,10]}]\n",
    "                     \n",
    "```\n",
    "- What are the best parameters found to maximize the area under the curve in each case?\n",
    "\n",
    "**e)** Discuss the results and curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "\n",
    "IMDIR = '101_ObjectCategories/'\n",
    "\n",
    "labelNamesAll = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(IMDIR):\n",
    "    labelNamesAll.append(dirnames)\n",
    "\n",
    "labelNamesAll = labelNamesAll[0]\n",
    "print(labelNamesAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imWidth = 100\n",
    "imHeight = 100\n",
    "N = 100\n",
    "\n",
    "X = np.zeros([2*N,imHeight*imWidth])\n",
    "Y = -np.ones([2*N,1])\n",
    "labelNames = ['dragonfly', 'crayfish']\n",
    "\n",
    "globalCount = 0\n",
    "for (i, label) in enumerate(labelNames):\n",
    "    filedir = os.path.join(IMDIR,label)\n",
    "\n",
    "    for filename in os.listdir(filedir):\n",
    "        f = os.path.join(filedir, filename)\n",
    "        if f.endswith('.jpg') and globalCount <= N:\n",
    "            image = skimage.io.imread(f, as_gray=True)\n",
    "            image = skimage.transform.resize(image, [imHeight,imWidth],mode='constant')\n",
    "            X[globalCount,:] = image.flatten()\n",
    "            Y[globalCount,:] = i\n",
    "            globalCount += 1\n",
    "\n",
    "print(\"Total number of samples\",globalCount)\n",
    "X = X[:globalCount,:]\n",
    "Y = Y[:globalCount,:]\n",
    "\n",
    "#Check the stored classes\n",
    "print(\"used labels\",labelNames)\n",
    "print(\"Size of data matrix\", X.shape)\n",
    "print(\"clas labels\", Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "Ntrain = np.rint(.75*Y.shape[0]).astype(int)\n",
    "Ntest = Y.shape[0]-Ntrain\n",
    "print('Training with', Ntrain , 'training samples and ', Ntest, 'testing samples.')\n",
    "\n",
    "# Randomize the order of X and Y\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "# Split the data and labels into training/testing sets\n",
    "X_train = X[0:Ntrain,:]\n",
    "Y_train = Y[0:Ntrain,:]\n",
    "\n",
    "X_test = X[Ntrain:,:]\n",
    "Y_test = Y[Ntrain:,:]\n",
    "\n",
    "print(\"size of train dataset\",X_train.shape)\n",
    "print(\"size of test dataset\",X_test.shape)\n",
    "print(\"train target vector\",Y_train.T)\n",
    "print(\"test target vector\",Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfs = {\n",
    "    \"SVM\": SVC(random_state=42), \n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42), \n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for (name, clf) in clfs.items():\n",
    "    print(name)\n",
    "    clf.fit(X_train, Y_train.ravel())\n",
    "\n",
    "    Y_pred = clf.predict(X_train)\n",
    "    print(\"Training accuray:\", accuracy_score(Y_train.ravel(), Y_pred))\n",
    "\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    print(\"Testing accuracy:\", accuracy_score(Y_test.ravel(), Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "Y_score = clfs['SVM'].decision_function(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "plt.plot(fpr, tpr, label=\"SVM (area = %0.2f)\" % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "Y_score = clfs['Decision Tree'].predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Tree (area = %0.2f)\" % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "Y_score = clfs['Random Forest'].predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Forest (area = %0.2f)\" % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('ROC curves')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "C_range = np.logspace(-12,3,16)\n",
    "gamma_range = np.logspace(-12,0,13)\n",
    "\n",
    "parametres = {'C': C_range, 'gamma': gamma_range,'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "\n",
    "grid = GridSearchCV(SVC(random_state=42), parametres, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "clf_svm = grid.best_estimator_\n",
    "clf_svm.fit(X_train, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [3, 5, 10, 15], \n",
    "              'max_features': [1, 10, 100, 1000, 10000],\n",
    "              'min_samples_leaf': [3, 5, 10, 15]}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), parameters, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "clf_tree = grid.best_estimator_\n",
    "clf_tree.fit(X_train, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2, 3, 5, 10], \n",
    "              'max_features': [50, 100, 200],\n",
    "              'n_estimators': [10, 30, 50, 70],\n",
    "              'min_samples_leaf': [3, 5, 7]}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), parameters, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "clf_forest = grid.best_estimator_\n",
    "clf_forest.fit(X_train, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_score = clf_svm.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "plt.plot(fpr, tpr, label=\"SVM (area = %0.2f)\" % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "Y_score = clf_tree.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Tree (area = %0.2f)\" % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "Y_score = clf_forest.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"Forest (area = %0.2f)\" % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('ROC curves')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observ alors que l'optimisation des hyper-paramètres a permis d'améliorer le score `roc_auc` pour lres trois modèles considerés. Cette amélioration est particulièrement notable pour les modèles SVM et arbre de décision. On obtient alors des scores aux alentours de 0.75 pour les 3 modèles, aucun ne s'est clairement demarqué des autres. Ces scores sont satisfaisants mais pour les améliorer on pourait par exemple continuer l'optimisation des paramètres avec des choix plus nombreux et précis des paramètres."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "widgets": {
   "state": {
    "933f9844aaa743b2b15f9b99c45e3c12": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
