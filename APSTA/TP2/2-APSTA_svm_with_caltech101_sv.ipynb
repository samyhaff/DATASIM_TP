{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0akOLV7gDyI"
   },
   "source": [
    "# 02 - Kernel methods and SVMs\n",
    "**Ecole Centrale Nantes**\n",
    "\n",
    "**Diana Mateus**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eEram8wgDyN"
   },
   "source": [
    "PARTICIPANTS: **Yassine Jamoud, Samy Haffoudhi**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9W_GTkrgDyQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage import io\n",
    "import random\n",
    "\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1swaFY5gDya"
   },
   "source": [
    "# 1. Image classification on Caltech 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVy0KgtHgDye"
   },
   "source": [
    "**a)** Download images from\n",
    "http://www.vision.caltech.edu/feifeili/Datasets.htm\n",
    "and run the code bellow to check the files and store the name of the classes in the list ```labelNamesAll```\n",
    "\n",
    "(Just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OJMOUCJgDyf"
   },
   "outputs": [],
   "source": [
    "## VERIFY LOCATION AND STORE LABEL NAMES\n",
    "\n",
    "IMDIR = '101_ObjectCategories/'\n",
    "\n",
    "\n",
    "labelNamesAll = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(IMDIR):\n",
    "    labelNamesAll.append(dirnames)\n",
    "    #uncomment to check what is found in this folder\n",
    "    #for filename in filenames:\n",
    "        #f = os.path.join(root, filename)\n",
    "        #if f.endswith(('.png', '.jpg', '.jpeg','.JPG', '.tif', '.gif')):\n",
    "        #    print(f)\n",
    "\n",
    "labelNamesAll = labelNamesAll[0]\n",
    "\n",
    "#The list of all labels/directories is\n",
    "print(labelNamesAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcNOyD3OgDym"
   },
   "source": [
    "**b. Build a reduced dataset for accelerating process.** To do so: \n",
    "- Consider only up to $K$ randomly drawn categories (start with a binary case)\n",
    "- Read only up to $N$ images for each class\n",
    "- Resize the images to $(imWidth*imHeight)$\n",
    "\n",
    "The dataset should consist of a \n",
    "- Input matrix $\\mathbf{X}$ of size $(K\\cdot N)\\times (imWidth\\cdot imHeight)$ with one image in every row of the matrix. \n",
    "- Output vector $\\mathbf{y}$ of size $(K\\cdot N)\\times 1$ with the label index of each input point in $\\bf X$.\n",
    "- the reduced list of the label names of size $K$ to map between the indices and the names.\n",
    "\n",
    "**Note than different classes may have different number of images so that the actual number of $\\bf X$ and $\\bf y$ is less than $K*N$**\n",
    "\n",
    "(Run and try to understand the structure of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqiN4dtWgDyn"
   },
   "outputs": [],
   "source": [
    "#build DATASET from K categories and (up to) N images from category\n",
    "K = 3 \n",
    "N = 200\n",
    "imWidth = 100 #resize images\n",
    "imHeight = 100\n",
    "\n",
    "#selection of label indices\n",
    "X = np.zeros([K*N,imHeight*imWidth]) #data matrix, one image per row\n",
    "#Y = np.zeros([K*N,1]) #label indices\n",
    "Y = -np.ones([K*N,1]) #label indices\n",
    "labelNames = []\n",
    "\n",
    "random.seed(a=42) #uncomment to make errors reproducible/comment to see variability\n",
    "\n",
    "globalCount = 0\n",
    "for i in range(K): \n",
    "    while True:\n",
    "        lab = random.randint(0,len(labelNamesAll)-1)\n",
    "        if lab not in labelNames:\n",
    "            break\n",
    "    #folders are named after the class label\n",
    "    filedir = os.path.join(IMDIR,labelNamesAll[lab])\n",
    "    print(filedir)\n",
    "\n",
    "    #save the name of the class\n",
    "    labelNames.append(labelNamesAll[lab])       \n",
    "\n",
    "    classCount = 0\n",
    "    for filename in os.listdir(filedir):\n",
    "        f = os.path.join(filedir, filename)\n",
    "        if f.endswith(('.jpg')) and (classCount < N):\n",
    "            #image = skimage.io.imread(f, as_grey=True) #Try this line instead of the one below if there is an error\n",
    "            image = skimage.io.imread(f, as_gray=True)\n",
    "            image = skimage.transform.resize(image, [imHeight,imWidth],mode='constant')#,anti_aliasing=True)\n",
    "            X[globalCount,:] = image.flatten()\n",
    "            Y[globalCount,:] = i\n",
    "            globalCount += 1\n",
    "            classCount += 1\n",
    "\n",
    "#Remove the unused entries of X and Y\n",
    "print(\"Total number of samples\",globalCount)\n",
    "X = X[:globalCount,:]\n",
    "Y = Y[:globalCount,:]\n",
    "\n",
    "#Check the stored classes\n",
    "print(\"used labels\",labelNames)\n",
    "print(\"Size of data matrix\", X.shape)\n",
    "print(\"clas labels\", Y.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjWhpLbDgDyr"
   },
   "source": [
    "**c**. Split the dataset into train (80% of samples) and test (20% samples). \n",
    "(Run and try to understand the structure of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYSNeplggDyt"
   },
   "outputs": [],
   "source": [
    "# Split in Train and test set with 80% - 20% rule\n",
    "\n",
    "Ntrain = np.rint(.8*Y.shape[0]).astype(int)\n",
    "Ntest = Y.shape[0]-Ntrain\n",
    "print('Training with', Ntrain , 'training samples and ', Ntest, 'testing samples.')\n",
    "\n",
    "# Randomize the order of X and Y\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "\n",
    "# Split the data and labels into training/testing sets\n",
    "X_train = X[0:Ntrain,:]\n",
    "Y_train = Y[0:Ntrain,:]\n",
    "\n",
    "X_test = X[Ntrain:,:]\n",
    "Y_test = Y[Ntrain:,:]\n",
    "\n",
    "print(\"size of train dataset\",X_train.shape)\n",
    "print(\"size of test dataset\",X_test.shape)\n",
    "print(\"train target vector\",Y_train.T)\n",
    "print(\"test target vector\",Y_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFsTEZ75gDyx"
   },
   "source": [
    "**d)** Training and testing a SVM\n",
    "- Create an SVC model using the sklearn module, \n",
    "- train it on the train set, \n",
    "- and test it on the test set**. \n",
    "\n",
    "(Fill in the code and answer the questions)\n",
    "\n",
    "**Question** SVMs are intrinsically binary classifiers, can you train the SVC for K>2? How is that achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBn8dDRHkBGb"
   },
   "source": [
    "**ANSWER**: On peut avoir recourt aux méthodes :\n",
    "* One VS One : on entraine $k(k-1)/2$ SVM et on choisit par vote majoritaire\n",
    "* One VS Rest: on entraine $k$ SVM et on sélectionne le score le plus élevé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p90Ezm7kgDyy"
   },
   "outputs": [],
   "source": [
    "# Create, train and test an svm model using the sklearn SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, Y_train.ravel())\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"True classes\",Y_test.T)\n",
    "print(\"Predictions\",Y_pred)\n",
    "errors = np.sum((Y_test.ravel()!=Y_pred))\n",
    "print('There were ', errors, 'errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mRLndBOgDy3"
   },
   "source": [
    "**e) Fill in the functions bellow to computing different evaluation measures and give a performance report**\n",
    "Look at the formulas and definitions in https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n",
    "\n",
    "Start by computing the confusion matrix, and the values TP, TN, FP, FN, for a binary case. When considering multiple clases ($K>2$) treat one class at a time as the postive class, and the remaining classes as negative. You may want to indicate the positive class as a parameter to the indicator function.\n",
    "\n",
    "**Question:** There are three ways of resuming the scores for a multiple class problem $K>2$, namely, the macroaverage, the microaverage and the weighted average. Implement and EXPLAIN them below.\n",
    "\n",
    "**Hint** Add a numerical zero eps to the denominators to prevent dividing by zero\n",
    "\n",
    "**Hint2** for the multi-class case:\n",
    "\n",
    "https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9pGgPazkVtQ"
   },
   "source": [
    "**ANSWER** Write your answer in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLy0E9SggDy7"
   },
   "outputs": [],
   "source": [
    "# Functions to compute the errors between prediction and ground truth \n",
    "\n",
    "def compute_measures(Y_gt,Y_pred, positiveClass=1): #Y_gt = ground truth\n",
    "    measures = dict()\n",
    "    \n",
    "    eps = 1e-12\n",
    "    \n",
    "    TP = TN = FP = FN = 0 \n",
    "    for pred, gt in zip(Y_pred, Y_gt):\n",
    "        if pred != positiveClass and gt != positiveClass:\n",
    "            TN += 1\n",
    "        elif pred == gt and gt == positiveClass:\n",
    "            TP += 1\n",
    "        elif pred != gt and gt == positiveClass:\n",
    "            FN += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    \n",
    "    print('TP ', TP, 'TN ', TN, 'FP ', FP, 'FN ', FN, 'Total', TP+TN+FP+FN)\n",
    "    measures['TP'] = TP\n",
    "    measures['TN'] = TN\n",
    "    measures['FP'] = FP\n",
    "    measures['FN'] = FN\n",
    "    \n",
    "    \n",
    "    # Accuracy\n",
    "    measures['accuracy'] = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    # Precision\n",
    "    measures['precision'] = TP / (TP + FP + eps)\n",
    "        \n",
    "    # Specificity\n",
    "    measures['specificity'] = FP / (TN + FP + eps)\n",
    "    \n",
    "    # Recall\n",
    "    measures['recall'] = TP / (TP + FN + eps)\n",
    "    \n",
    "    # F-measure\n",
    "    measures['f1'] = 2 * TP / (2 * TP + FP + FN + eps)\n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    measures['npv'] = TN / (TN + FN + eps)\n",
    "    \n",
    "    # False Predictive Value\n",
    "    measures['fpr'] = FN / (FN + TN + eps)\n",
    "    \n",
    "    print('Accuracy ', measures['accuracy'], '\\n',\n",
    "          'Precision', measures['precision'], '\\n',\n",
    "          'Recall', measures['recall'], '\\n',\n",
    "          'Specificity ', measures['specificity'], '\\n',\n",
    "          'F-measure', measures['f1'], '\\n',\n",
    "          'NPV', measures['npv'],'\\n',\n",
    "          'FPV', measures['fpr'],'\\n')\n",
    "\n",
    "    return measures\n",
    "\n",
    "def micro_average(measuresList):\n",
    "    microAverage = dict()\n",
    "    eps = 1e-12\n",
    "    \n",
    "    TP = np.sum([measures['TP'] for measures in measuresList])\n",
    "    FP = np.sum([measures['FP'] for measures in measuresList])\n",
    "    TN = np.sum([measures['TN'] for measures in measuresList])\n",
    "    FN = np.sum([measures['FN'] for measures in measuresList])\n",
    "    \n",
    "    # Accuracy\n",
    "    microAverage['accuracy'] = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    # Precision\n",
    "    microAverage['precision'] = TP / (TP + FP + eps)\n",
    "        \n",
    "    # Specificity\n",
    "    microAverage['specificity'] = TN / (TN + FP + eps)\n",
    "    \n",
    "    # Recall\n",
    "    microAverage['recall'] = TP / (TP + FN + eps)\n",
    "    \n",
    "    # F-measure\n",
    "    microAverage['f1'] = 2 * TP / (2 * TP + FP + FN + eps)\n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    microAverage['npv'] = TN / (TN + FN + eps)\n",
    "    \n",
    "    # False Predictive Value\n",
    "    microAverage['fpr'] = FN / (FN + TN + eps)\n",
    "        \n",
    "    print('Accuracy ', microAverage['accuracy'], '\\n',\n",
    "          'Precision', microAverage['precision'], '\\n',\n",
    "          'Recall', microAverage['recall'], '\\n',\n",
    "          'Specificity ', microAverage['specificity'], '\\n',\n",
    "          'F-measure', microAverage['f1'], '\\n',\n",
    "          'NPV', microAverage['npv'],'\\n',\n",
    "          'FPV', microAverage['fpr'],'\\n')\n",
    "    \n",
    "    return microAverage\n",
    "\n",
    "def macro_average(measuresList):\n",
    "    macroAverage = dict()\n",
    "\n",
    "    # Accuracy\n",
    "    macroAverage['accuracy'] = np.average([measure['accuracy'] for measure in measuresList])\n",
    "    \n",
    "    # Precision\n",
    "    macroAverage['precision'] = np.average([measure['precision'] for measure in measuresList])\n",
    "        \n",
    "    # Specificity\n",
    "    macroAverage['specificity'] = np.average([measure['specificity'] for measure in measuresList])\n",
    "    \n",
    "    # Recall\n",
    "    macroAverage['recall'] = np.average([measure['recall'] for measure in measuresList])\n",
    "    \n",
    "    # F-measure\n",
    "    macroAverage['f1'] = np.average([measure['f1'] for measure in measuresList])\n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    macroAverage['npv'] = np.average([measure['npv'] for measure in measuresList])\n",
    "    \n",
    "    # False Predictive Value\n",
    "    macroAverage['fpr'] = np.average([measure['fpr'] for measure in measuresList])\n",
    "    \n",
    "    print('Accuracy ', macroAverage['accuracy'], '\\n',\n",
    "          'Precision', macroAverage['precision'], '\\n',\n",
    "          'Recall', macroAverage['recall'], '\\n',\n",
    "          'Specificity ', macroAverage['specificity'], '\\n',\n",
    "          'F-measure', macroAverage['f1'], '\\n',\n",
    "          'NPV', macroAverage['npv'],'\\n',\n",
    "          'FPV', macroAverage['fpr'],'\\n')\n",
    "    \n",
    "    return macroAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4qj336qgDzA"
   },
   "source": [
    "**e)** Measure the performance of the SVC model for multiple classes $K>2$\n",
    "\n",
    "First collect the measures when considering each class as positive, then, compute macro and microaverage \n",
    "\n",
    "Compare your results to those of sklearn metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTFlxl5dgDzA"
   },
   "outputs": [],
   "source": [
    "#Fill in a list of measure dictionaries taking as input a different positive class\n",
    "\n",
    "multiclass = []\n",
    "for k in range(K):\n",
    "    print('For class',labelNames[k])\n",
    "    multiclass.append(compute_measures(Y_test.ravel(),Y_pred, positiveClass=k))\n",
    "\n",
    "print('Macro-average')\n",
    "macro_average(multiclass)\n",
    "    \n",
    "print('Micro-average')\n",
    "micro_average(multiclass)\n",
    "\n",
    "from sklearn.metrics import classification_report #confusion_matrix, accuracy_score, precision_score, recall_score, f1_micro, f1_macro\n",
    "print(classification_report(Y_test.ravel(), Y_pred, target_names=labelNames, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe qu'on obtient les même résultats que ceux fournis par scikit-learn.\n",
    "Les résultats micro-average ne sont pas affichés car ils correspondent à l'accuracy qui est-elle déjà affichée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrjhvsm0gDzF"
   },
   "source": [
    "**f) Show the test images as well as the the predictions (Y_pred) vs the ground truth (Y_gt) labels for the best model**\n",
    "(Just run for each analysed model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7NW94MJgDzF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show some results\n",
    "width=20\n",
    "height=15\n",
    "plt.rcParams['figure.figsize'] = [width, height]\n",
    "fig=plt.figure()\n",
    "imCounter = 1\n",
    "for i in range(len(Y_test)):\n",
    "    image=np.reshape(X_test[i,:], (imHeight,imWidth)) \n",
    "\n",
    "    plt.subplot(5,7,imCounter)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    gtLabel = labelNames[Y_test.ravel()[i].astype(int)]\n",
    "    predLabel = labelNames[Y_pred.ravel()[i].astype(int)]\n",
    "    plt.title('GT: {}. \\n Pred: {}'.format(gtLabel, predLabel))\n",
    "\n",
    "    imCounter += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkhaiIapgDzJ"
   },
   "source": [
    "**g) REPORT:**  Change the kernel and other hyperparameters of your SVC trying to optimize the F1 measure for different cases. Describe in your report the different variants of the model tried. You may want to split your dataset into train, validation and test sets this time to find the best hyperparameters. Present and discuss your findings for different hyperparameters, number of classes and numbers of images. THIS IS THE MOST IMPORTANT PART FOR THE EVALUATION. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Méthodologie :**\n",
    "\n",
    "* on choisit d'optimiser le score f1_micro, plus adapté lorsqu'on dispose d'un nombre d'exemples variants en fonction des classes\n",
    "* On divise le dataset en 3 parties : test, validation et entrainement\n",
    "* On commence par sélectionner le noyau. Pour ce faire, on se contente d'utiliser des hyperparamètres par défaut et de choisir le noyau au meilleur score f1_micro sur le dataset de validation\n",
    "* On passe alors à l'optimisation des hyper-paramètres pour le(s) modèle(s) retenu(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCt_nfe0qEVx"
   },
   "outputs": [],
   "source": [
    "# Séparation du dataset en 3 parties : test, validation et entrainement\n",
    "\n",
    "Ntrain = np.rint(.8*Y.shape[0]).astype(int)\n",
    "Ntest = Nvalid = (Y.shape[0]-Ntrain) // 2\n",
    "print('Training with', Ntrain , 'training samples, ', Nvalid, 'validation samples and ', Ntest, 'testing samples')\n",
    "\n",
    "# Randomize the order of X and Y\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "# Split the data and labels into training/testing sets\n",
    "X_train = X[0:Ntrain,:]\n",
    "Y_train = Y[0:Ntrain,:]\n",
    "\n",
    "X_valid = X[Ntrain:Ntrain+Nvalid,:]\n",
    "Y_valid = Y[Ntrain:Ntrain+Nvalid,:]\n",
    "\n",
    "X_test = X[Ntrain+Nvalid:,:]\n",
    "Y_test = Y[Ntrain+Nvalid:,:]\n",
    "\n",
    "print(\"size of train dataset\",X_train.shape)\n",
    "print(\"size of validation dataset\",X_valid.shape)\n",
    "print(\"size of test dataset\",X_test.shape)\n",
    "print(\"train target vector\",Y_train.T)\n",
    "print(\"validation target vector\",Y_valid.T)\n",
    "print(\"test target vector\",Y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix du kernel le plus prometteur\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "\n",
    "for k in kernels:\n",
    "    clf = SVC(kernel=k)\n",
    "    clf.fit(X_train, Y_train.ravel())\n",
    "    Y_pred = clf.predict(X_valid)\n",
    "    print(f\"{k} kernel score: {f1_score(Y_valid, Y_pred, average='micro')}\")\n",
    "    # print(classification_report(Y_valid.ravel(), Y_pred, target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.logspace(-3,10,10)\n",
    "gamma_values = np.logspace(-9,3,10)\n",
    "\n",
    "clfs = [SVC()] * 4\n",
    "f1_best = [0] * 4\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "for C in C_values: \n",
    "    for gamma in gamma_values:\n",
    "        for i in range(4):\n",
    "            clf = SVC(kernel=kernels[i], C=C, gamma=gamma)\n",
    "            clf.fit(X_train, Y_train.ravel())\n",
    "            Y_pred = clf.predict(X_valid)\n",
    "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
    "            if f1 > f1_best[i]:\n",
    "                clfs[i] = clf\n",
    "                f1_best[i] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 MICRO SCORES:')\n",
    "\n",
    "for i in range(4):\n",
    "    print(kernels[i], f1_best[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 3 modèles permettent d'obtenir un bon score f1_micro d'environ 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 MICRO SCORES:')\n",
    "\n",
    "for i in range(4):\n",
    "    clf = clfs[i]\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    print(kernels[i], f1_score(Y_test, Y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :**\n",
    "\n",
    "* Pour 3 classes et en utilisant toutes les images disponibles\n",
    "    * on arrive à obtenir de bons scores sur les données de validation mais on n'obtient de bien plus mauvais scores sur les données de test.\n",
    "* Pour 3 classes et en utilisant un maximum de 20 images par classes\n",
    "    * on obtient de moins bon scores f1_micro que précedemment, comme attendu\n",
    "* Pour 2 classes et en utilisant un maximum de 20 images par classes\n",
    "    * les scores des différents kernels sont plus porches les uns des autres que précedemment\n",
    "* Pour 2 classes et en utilisant toutes les images disponibles\n",
    "    * on obtient des scores meilleurs que pour le cas précedent\n",
    "    * les 4 kernels obtiennent tous le même score  Pour le cas à 2 classes on voit alors que le choix du kernel a moins d'importance que pour celui à 3 classes\n",
    "* Pour 5 classes et toutes les images\n",
    "    * on observe à nouveau cette fois une différence entre les 4 kernels\n",
    "    * les scores sont inférieurs à ceux précedents\n",
    "    \n",
    "Donc,\n",
    "* Plus on utilise d'images, plus les scores seront élevés\n",
    "* Plus on utilise de classes différentes, plus le problème devient compliqué et plus le cohoix des paramètres du modèle et le temps de calcul deviennent importants (plus de SVM à entrainer et nécessité de plus de données)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "02-ARTIN-svm-with-caltech101-sv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
