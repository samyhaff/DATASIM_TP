{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0akOLV7gDyI"
   },
   "source": [
    "# 02 - Kernel methods and SVMs\n",
    "** Ecole Centrale Nantes **\n",
    "\n",
    "** Diana Mateus **\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eEram8wgDyN"
   },
   "source": [
    "PARTICIPANTS: **(Fill in your names)**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N9W_GTkrgDyQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage import io\n",
    "import random\n",
    "\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1swaFY5gDya"
   },
   "source": [
    "# 1. Image classification on Caltech 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVy0KgtHgDye"
   },
   "source": [
    "**a)** Download images from\n",
    "http://www.vision.caltech.edu/feifeili/Datasets.htm\n",
    "and run the code bellow to check the files and store the name of the classes in the list ```labelNamesAll```\n",
    "\n",
    "(Just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3OJMOUCJgDyf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accordion', 'airplanes', 'anchor', 'ant', 'BACKGROUND_Google', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'Faces', 'Faces_easy', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'Leopards', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'Motorbikes', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']\n"
     ]
    }
   ],
   "source": [
    "## VERIFY LOCATION AND STORE LABEL NAMES\n",
    "\n",
    "IMDIR = '101_ObjectCategories/'\n",
    "\n",
    "\n",
    "labelNamesAll = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(IMDIR):\n",
    "    labelNamesAll.append(dirnames)\n",
    "    #uncomment to check what is found in this folder\n",
    "    #for filename in filenames:\n",
    "        #f = os.path.join(root, filename)\n",
    "        #if f.endswith(('.png', '.jpg', '.jpeg','.JPG', '.tif', '.gif')):\n",
    "        #    print(f)\n",
    "\n",
    "labelNamesAll = labelNamesAll[0]\n",
    "\n",
    "#The list of all labels/directories is\n",
    "print(labelNamesAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcNOyD3OgDym"
   },
   "source": [
    "**b. Build a reduced dataset for accelerating process.** To do so: \n",
    "- Consider only up to $K$ randomly drawn categories (start with a binary case)\n",
    "- Read only up to $N$ images for each class\n",
    "- Resize the images to $(imWidth*imHeight)$\n",
    "\n",
    "The dataset should consist of a \n",
    "- Input matrix $\\mathbf{X}$ of size $(K\\cdot N)\\times (imWidth\\cdot imHeight)$ with one image in every row of the matrix. \n",
    "- Output vector $\\mathbf{y}$ of size $(K\\cdot N)\\times 1$ with the label index of each input point in $\\bf X$.\n",
    "- the reduced list of the label names of size $K$ to map between the indices and the names.\n",
    "\n",
    "**Note than different classes may have different number of images so that the actual number of $\\bf X$ and $\\bf y$ is less than $K*N$**\n",
    "\n",
    "(Run and try to understand the structure of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oqiN4dtWgDyn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101_ObjectCategories/scissors\n",
      "101_ObjectCategories/camera\n",
      "Total number of samples 79\n",
      "used labels ['scissors', 'camera']\n",
      "Size of data matrix (79, 10000)\n",
      "clas labels [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#build DATASET from K categories and (up to) N images from category\n",
    "K = 2 \n",
    "N = 40\n",
    "imWidth = 100 #resize images\n",
    "imHeight = 100\n",
    "\n",
    "#selection of label indices\n",
    "X = np.zeros([K*N,imHeight*imWidth]) #data matrix, one image per row\n",
    "#Y = np.zeros([K*N,1]) #label indices\n",
    "Y = -np.ones([K*N,1]) #label indices\n",
    "labelNames = []\n",
    "\n",
    "random.seed(a=42) #uncomment to make errors reproducible/comment to see variability\n",
    "\n",
    "globalCount = 0\n",
    "for i in range(K): \n",
    "    while True:\n",
    "        lab = random.randint(0,len(labelNamesAll)-1)\n",
    "        if lab not in labelNames:\n",
    "            break\n",
    "    #folders are named after the class label\n",
    "    filedir = os.path.join(IMDIR,labelNamesAll[lab])\n",
    "    print(filedir)\n",
    "\n",
    "    #save the name of the class\n",
    "    labelNames.append(labelNamesAll[lab])       \n",
    "\n",
    "    classCount = 0\n",
    "    for filename in os.listdir(filedir):\n",
    "        f = os.path.join(filedir, filename)\n",
    "        if f.endswith(('.jpg')) and (classCount < N):\n",
    "            #image = skimage.io.imread(f, as_grey=True) #Try this line instead of the one below if there is an error\n",
    "            image = skimage.io.imread(f, as_gray=True)\n",
    "            image = skimage.transform.resize(image, [imHeight,imWidth],mode='constant')#,anti_aliasing=True)\n",
    "            X[globalCount,:] = image.flatten()\n",
    "            Y[globalCount,:] = i\n",
    "            globalCount += 1\n",
    "            classCount += 1\n",
    "\n",
    "#Remove the unused entries of X and Y\n",
    "print(\"Total number of samples\",globalCount)\n",
    "X = X[:globalCount,:]\n",
    "Y = Y[:globalCount,:]\n",
    "\n",
    "#Check the stored classes\n",
    "print(\"used labels\",labelNames)\n",
    "print(\"Size of data matrix\", X.shape)\n",
    "print(\"clas labels\", Y.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjWhpLbDgDyr"
   },
   "source": [
    "**c**. Split the dataset into train (80% of samples) and test (20% samples). \n",
    "(Run and try to understand the structure of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XYSNeplggDyt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 63 training samples and  16 testing samples.\n",
      "size of train dataset (63, 10000)\n",
      "size of test dataset (16, 10000)\n",
      "train target vector [[1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      "  1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0.]]\n",
      "test target vector [[0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Split in Train and test set with 80% - 20% rule\n",
    "\n",
    "Ntrain = np.rint(.8*Y.shape[0]).astype(int)\n",
    "Ntest = Y.shape[0]-Ntrain\n",
    "print('Training with', Ntrain , 'training samples and ', Ntest, 'testing samples.')\n",
    "\n",
    "# Randomize the order of X and Y\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "\n",
    "# Split the data and labels into training/testing sets\n",
    "X_train = X[0:Ntrain,:]\n",
    "Y_train = Y[0:Ntrain,:]\n",
    "\n",
    "X_test = X[Ntrain:,:]\n",
    "Y_test = Y[Ntrain:,:]\n",
    "\n",
    "print(\"size of train dataset\",X_train.shape)\n",
    "print(\"size of test dataset\",X_test.shape)\n",
    "print(\"train target vector\",Y_train.T)\n",
    "print(\"test target vector\",Y_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFsTEZ75gDyx"
   },
   "source": [
    "**d) Training and testing a SVM\n",
    "- Create an SVC model using the sklearn module, \n",
    "- train it on the train set, \n",
    "- and test it on the test set**. \n",
    "\n",
    "(Fill in the code and answer the questions)\n",
    "\n",
    "**Question** SVMs are intrinsically binary classifiers, can you train the SVC for K>2? How is that achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBn8dDRHkBGb"
   },
   "source": [
    "**ANSWER**: Fill your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "p90Ezm7kgDyy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True classes [[0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.]]\n",
      "Predictions [0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "There were  2 errors\n"
     ]
    }
   ],
   "source": [
    "# Create, train and test an svm model using the sklearn SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, Y_train.ravel())\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"True classes\",Y_test.T)\n",
    "print(\"Predictions\",Y_pred)\n",
    "errors = np.sum((Y_test.ravel()!=Y_pred))\n",
    "print('There were ', errors, 'errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mRLndBOgDy3"
   },
   "source": [
    "**e) Fill in the functions bellow to computing different evaluation measures and give a performance report**\n",
    "Look at the formulas and definitions in https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n",
    "\n",
    "Start by computing the confusion matrix, and the values TP, TN, FP, FN, for a binary case. When considering multiple clases ($K>2$) treat one class at a time as the postive class, and the remaining classes as negative. You may want to indicate the positive class as a parameter to the indicator function.\n",
    "\n",
    "**Question:** There are three ways of resuming the scores for a multiple class problem $K>2$, namely, the macroaverage, the microaverage and the weighted average. Implement and EXPLAIN them below.\n",
    "\n",
    "**Hint** Add a numerical zero eps to the denominators to prevent dividing by zero\n",
    "\n",
    "**Hint2** for the multi-class case:\n",
    "\n",
    "https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9pGgPazkVtQ"
   },
   "source": [
    "**ANSWER** Write your answer in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLy0E9SggDy7"
   },
   "outputs": [],
   "source": [
    "# Functions to compute the errors between prediction and ground truth \n",
    "\n",
    "def compute_measures(Y_gt,Y_pred, positiveClass=1): #Y_gt = ground truth\n",
    "    measures = dict()\n",
    "    Y_len = len(Y_gt)\n",
    "    \n",
    "    eps = 1e-12\n",
    "    \n",
    "    # True positives TP\n",
    "    TP = np.sum((Y_test.ravel()!=Y_pred))\n",
    "        \n",
    "    # True negatives TN\n",
    "    TN = \n",
    "    \n",
    "    # False positives FP\n",
    "    FP = \n",
    "        \n",
    "    # False negatives FN\n",
    "    FN = \n",
    "\n",
    "    print('TP ', TP, 'TN ', TN, 'FP', FP, 'FN', FN, 'Total', TP+TN+FP+FN)\n",
    "    measures['TP'] = TP\n",
    "    measures['TN'] = TN\n",
    "    measures['FP'] = FP\n",
    "    measures['FN'] = FN\n",
    "    \n",
    "    \n",
    "    # Accuracy\n",
    "    measures['accuracy'] = \n",
    "    \n",
    "    # Precision\n",
    "    measures['precision'] = \n",
    "        \n",
    "    # Specificity\n",
    "    measures['specificity']=\n",
    "    \n",
    "    # Recall\n",
    "    measures['recall'] = \n",
    "    \n",
    "    # F-measure\n",
    "    measures['f1'] = \n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    measures['npv'] = \n",
    "    \n",
    "    # False Predictive Value\n",
    "    measures['fpr'] = \n",
    "    \n",
    "    print('Accuracy ', measures['accuracy'], '\\n',\n",
    "          'Precision', measures['precision'], '\\n',\n",
    "          'Recall', measures['recall'], '\\n',\n",
    "          'Specificity ', measures['specificity'], '\\n',\n",
    "          'F-measure', measures['f1'], '\\n',\n",
    "          'NPV', measures['npv'],'\\n',\n",
    "          'FPV', measures['fpr'],'\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    return measures\n",
    "\n",
    "def micro_average(measuresList):\n",
    "    microAverage = dict()\n",
    "    eps = 1e-12\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Accuracy\n",
    "    microAverage['accuracy'] = \n",
    "    \n",
    "    # Precision\n",
    "    microAverage['precision'] = \n",
    "        \n",
    "    # Specificity\n",
    "    microAverage['specificity'] = \n",
    "    \n",
    "    # Recall\n",
    "    microAverage['recall'] = \n",
    "    \n",
    "    # F-measure\n",
    "    microAverage['f1'] = \n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    microAverage['npv'] = \n",
    "    \n",
    "    # False Predictive Value\n",
    "    microAverage['fpr'] = \n",
    "    \n",
    "        \n",
    "    print('Accuracy ', microAverage['accuracy'], '\\n',\n",
    "          'Precision', microAverage['precision'], '\\n',\n",
    "          'Recall', microAverage['recall'], '\\n',\n",
    "          'Specificity ', microAverage['specificity'], '\\n',\n",
    "          'F-measure', microAverage['f1'], '\\n',\n",
    "          'NPV', microAverage['npv'],'\\n',\n",
    "          'FPV', microAverage['fpr'],'\\n')\n",
    "    \n",
    "    return microAverage\n",
    "\n",
    "def macro_average(measuresList):\n",
    "    macroAverage = dict()\n",
    "\n",
    "    # Accuracy\n",
    "    macroAverage['accuracy'] = \n",
    "    \n",
    "    # Precision\n",
    "    macroAverage['precision'] = \n",
    "        \n",
    "    # Specificity\n",
    "    macroAverage['specificity']= \n",
    "    \n",
    "    # Recall\n",
    "    macroAverage['recall'] = \n",
    "    \n",
    "    # F-measure\n",
    "    macroAverage['f1'] = \n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    macroAverage['npv'] = \n",
    "    \n",
    "    # False Predictive Value\n",
    "    macroAverage['fpr'] = \n",
    "    \n",
    "    print('Accuracy ', macroAverage['accuracy'], '\\n',\n",
    "          'Precision', macroAverage['precision'], '\\n',\n",
    "          'Recall', macroAverage['recall'], '\\n',\n",
    "          'Specificity ', macroAverage['specificity'], '\\n',\n",
    "          'F-measure', macroAverage['f1'], '\\n',\n",
    "          'NPV', macroAverage['npv'],'\\n',\n",
    "          'FPV', macroAverage['fpr'],'\\n')\n",
    "    \n",
    "    return macroAverage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4qj336qgDzA"
   },
   "source": [
    "**e)** Measure the performance of the SVC model for multiple classes $K>2$\n",
    "\n",
    "First collect the measures when considering each class as positive, then, compute macro and microaverage \n",
    "\n",
    "Compare your results to those of sklearn metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTFlxl5dgDzA"
   },
   "outputs": [],
   "source": [
    "#Fill in a list of measure dictionaries taking as input a different positive class\n",
    "\n",
    "multiclass = []\n",
    "for k in range(K):\n",
    "    print('For class',labelNames[k])\n",
    "    multiclass.append(compute_measures(Y_test.ravel(),Y_pred, positiveClass=k))\n",
    "\n",
    "print('Macro-average')\n",
    "macro_average(multiclass)\n",
    "    \n",
    "print('Micro-average')\n",
    "micro_average(multiclass)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report #confusion_matrix, accuracy_score, precision_score, recall_score, f1_micro, f1_macro\n",
    "print(classification_report(Y_test.ravel(), Y_pred, target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrjhvsm0gDzF"
   },
   "source": [
    "**f) Show the test images as well as the the predictions (Y_pred) vs the ground truth (Y_gt) labels for the best model**\n",
    "(Just run for each analysed model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7NW94MJgDzF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show some results\n",
    "width=20\n",
    "height=15\n",
    "plt.rcParams['figure.figsize'] = [width, height]\n",
    "fig=plt.figure()\n",
    "imCounter = 1\n",
    "for i in range(len(Y_test)):\n",
    "    image=np.reshape(X_test[i,:], (imHeight,imWidth)) \n",
    "\n",
    "    plt.subplot(5,7,imCounter)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    gtLabel = labelNames[Y_test.ravel()[i].astype(int)]\n",
    "    predLabel = labelNames[Y_pred.ravel()[i].astype(int)]\n",
    "    plt.title('GT: {}. \\n Pred: {}'.format(gtLabel, predLabel))\n",
    "\n",
    "    imCounter += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkhaiIapgDzJ"
   },
   "source": [
    "** g) REPORT:**  Change the kernel and other hyperparameters of your SVC trying to optimize the F1 measure for different cases. Describe in your report the different variants of the model tried. You may want to split your dataset into train, validation and test sets this time to find the best hyperparameters. Present and discuss your findings for different hyperparameters, number of classes and numbers of images. THIS IS THE MOST IMPORTANT PART FOR THE EVALUATION. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCt_nfe0qEVx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "02-ARTIN-svm-with-caltech101-sv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
