{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks \n",
    "\n",
    "\n",
    "**Ecole Centrale Nantes**\n",
    "\n",
    "**Diana Mateus**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Participants :** Yassine Jamoud, Samy Haffoudhi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General description\n",
    "In this lab we will create a simple classifier based on neural networks. We will progress in two parts:\n",
    "- In the first part, and to better understand the involved operations, we will create a single-neuron model and optimize its parameters \"by hand\". For this first part we will only use the **Numpy** library\n",
    "- We will then build a multi-layer perceptron with the built-in library **Keras** module and **tensorflow**. Tensorflow is already installed in the university computers. If using your own computer you should have already installed **tensorflow** or use **collab** online platform.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "Start by runing the following lines to load and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('dataset/train_catvnoncat.h5', \"r\")\n",
    "    train_x = np.array(train_dataset[\"train_set_x\"][:]) \n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
    "    test_dataset = h5py.File('dataset/test_catvnoncat.h5', \"r\")\n",
    "    test_x = np.array(test_dataset[\"test_set_x\"][:]) \n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    train_y = train_y.reshape((1, train_y.shape[0]))\n",
    "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, classes\n",
    "\n",
    "train_x, train_y, test_x, test_y, classes=load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run several times to visualize different data points\n",
    "# the title shows the ground truth class labels (0=no cat , 1 = cat)\n",
    "index = np.random.randint(low=0,high=train_y.shape[1])\n",
    "plt.imshow(train_x[index])\n",
    "plt.title(\"Image \"+str(index)+\" label \"+str(train_y[0,index]))\n",
    "plt.show()\n",
    "print (\"Train X shape: \" + str(train_x.shape))\n",
    "print (\"We have \"+str(train_x.shape[0]), \n",
    "       \"images of dimensionality \" \n",
    "       + str(train_x.shape[1])+ \"x\"\n",
    "       + str(train_x.shape[2])+ \"x\"\n",
    "       + str(train_x.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "In the following lines we vectorize the images (Instead of a 2-D image we will give as input to the models a 1-D vector). The normalization makes the image intensities be between 0 and 1, and converts the images to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0], -1).T\n",
    "test_x = test_x.reshape(test_x.shape[0], -1).T\n",
    "print (\"Train X shape: \" + str(train_x.shape))\n",
    "print (\"Train Y shape: \" + str(train_y.shape))\n",
    "print (\"Test X shape: \" + str(test_x.shape))\n",
    "print (\"Test Y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/255.\n",
    "test_x = test_x/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification with a single neuron \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Fill-in the following three functions to define the single neuron model (a single neuron in the hidden layer):\n",
    "- A function **initialize_parameters** of the neuron. The function will randomly initializes the model's weights with small values. Initialize the bias with 0. What is the number of weights required? pass this information as a parameter to the function.\n",
    "- A function **sigmoid** that computes the sigmoid activation function\n",
    "- A function **neuron** that given an input vector, the weights and bias, computes the output of the single neuron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dim):\n",
    "    w = np.random.randn(dim, 1) * 0.001\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(w,b,X):\n",
    "    pred_y = sigmoid(np.matmul(w.T, X) + b)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** **Forward Pass:**\n",
    "Use the three functions above to compute a first forward pass for the input matrix $X$ containing the loaded dataset, for some initialization of the weights and bias.\n",
    " \n",
    " \\begin{align}\n",
    " Y_{\\rm pred}=\\sigma(w^\\top X+b) = [y_{\\rm pred}^{(1)},y_{\\rm pred}^{(2)},\\dots,y_{\\rm pred}^{(m)}]\n",
    " \\end{align}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = train_x.shape[0]\n",
    "w, b = initialize_parameters(dim)\n",
    "print(w.shape)\n",
    "pred_y = neuron(w, b, train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Cost estimation:**\n",
    " \n",
    "We will use a binary cross-entropy loss, so that the empirical risk can be computed as:\n",
    " \\begin{align}\n",
    " E = - \\frac{1}{m} \\sum_{i=1}^m \n",
    " y^{(i)} \\log(y_{\\rm pred}^{(i)}) +\n",
    " (1-y^{(i)}) \\log(1-y_{\\rm pred}^{(i)})\n",
    " \\end{align}\n",
    " \n",
    " The following cross-entropy function should give as result the scalar cost value computed over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(Y,Ypred):\n",
    "    eps = 10**(-12)\n",
    "    cost = -np.sum(Y * np.log(Ypred + eps) + (1 - Y) * np.log(1 - Ypred + eps)) / Ypred.shape[1]\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Back propagation:**\n",
    "\n",
    "After initializing the parameters and doing a forward pass, we need to backpropagate the cost by computing the gradient with respect to the model parameters to later update the weights\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w} = \\\\\n",
    "\\frac{\\partial E}{\\partial b} = \n",
    "\\end{align}\n",
    "\n",
    "See a demonstration of the gradient computation in \n",
    "https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "Fill-in the backpropagation function which receives as input the the training set (X,Y), as well as the current predictions and returns the gradients updates for the weights and bias\n",
    "\n",
    "Hint: When the error is computed for several samples simultaneously, the gradient is averaged over the contribution of different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(X, Y, Ypred):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    #find gradient (back propagation)\n",
    "    dw = np.matmul(X, (Ypred - Y).T) / m\n",
    "    db = np.sum(Ypred - Y) / m\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db} \n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backpropagate(train_x, train_y.ravel(), pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Optimization**\n",
    "After initializing the parameters, computing the cost function, and calculating gradients, we can now update the parameters using gradient descent. Use the functions implemented above to fill_in the \"gradient_descent\" function that optimizes the parameters given a training set X, Y, a fixed number of iterations, and a learning_rate. Store and plot the value of the loss function at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, iterations, learning_rate):\n",
    "    costs = []\n",
    "    w, b = initialize_parameters(X.shape[0])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        Ypred = neuron(w, b, X)\n",
    "        cost = crossentropy(Y, Ypred)\n",
    "        grads = backpropagate(X, Y, Ypred)\n",
    "        \n",
    "        #update parameters\n",
    "        w -= learning_rate * grads['dw']\n",
    "        b -= learning_rate * grads['db']\n",
    "        costs.append(cost)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "       \n",
    "    return w,b, costs\n",
    "\n",
    "w, b, costs = gradient_descent(train_x,train_y.ravel(),iterations=2000, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Plot the training curve**\n",
    "Plot the evolution of the cost vs the iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Prediction**\n",
    "Use the optimized parameters to make predictions both for the train and test sets and compute the accuracy for each. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):    \n",
    "    y_pred = neuron(w, b, X)\n",
    "    return y_pred\n",
    "\n",
    "# predict \n",
    "train_pred_y = predict(w, b, train_x)\n",
    "test_pred_y = predict(w, b, test_x)\n",
    "print(\"Train Acc: {} %\".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))\n",
    "print(\"Test Acc: {} %\".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) Early stopping** \n",
    "- Adapt the gradient descent function to consider part (a percentage) of the training data for validation. Use the validation set to choose the training hyperparameters (learning_rate, iterations). \n",
    "- Plot the training and validation curves\n",
    "- Report again the training and test accuracy and loss for the new trained model\n",
    "- What do you observe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_early_stopping(X, Y, iterations, learning_rate, percentage=.1):\n",
    "    max_ind = int(Y.shape[0] * (1 - percentage))\n",
    "    train_x, val_x = X[:, :max_ind], X[:, max_ind:]\n",
    "    train_y, val_y = Y[:max_ind], Y[max_ind:]\n",
    "    \n",
    "    train_costs, val_costs = [], []\n",
    "    w, b = initialize_parameters(train_x.shape[0])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        Ypred = neuron(w, b, val_x)\n",
    "        val_cost = crossentropy(val_y, Ypred)\n",
    "        \n",
    "        Ypred = neuron(w, b, train_x)\n",
    "        train_cost = crossentropy(train_y, Ypred)\n",
    "        grads = backpropagate(train_x, train_y, Ypred)\n",
    "        \n",
    "        #update parameters\n",
    "        w -= learning_rate * grads['dw']\n",
    "        b -= learning_rate * grads['db']\n",
    "       \n",
    "        train_costs.append(train_cost)\n",
    "        val_costs.append(val_cost)\n",
    "        \n",
    "        # if i % 100 == 0:\n",
    "        #     print (\"Train cost after iteration %i: %f\" %(i, train_cost))\n",
    "        #     print (\"Val cost after iteration %i: %f\" %(i, val_cost))\n",
    "    \n",
    "    return w, b, train_costs, val_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, train_costs, val_costs = gradient_descent_early_stopping(train_x, train_y.ravel(), 4000, learning_rate=0.00005)\n",
    "\n",
    "plt.plot(train_costs)\n",
    "plt.plot(val_costs)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylim([0, 0.7])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe qu'à partir d'environ 2000 itérations la courbe d'entrainement passe en dessous de celle de validation ce qui signifie qu'il y a overfitting. Pour la valeur de `learning_rate` utilisée en e) on observe que la courbe de validation commence à croitre rapidement, ce qui n'est pas souhaitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_y = predict(w, b, train_x)\n",
    "test_pred_y = predict(w, b, test_x)\n",
    "loss =  crossentropy(train_y, train_pred_y)\n",
    "print(\"Train Acc: {} %\".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))\n",
    "print(\"Test Acc: {} %\".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))\n",
    "print(\"Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En diminuant alors la valeur du `learning_rate`, on évite que la courbe de validation décole mais le learning rate devient alors insufisant pour diminuer la fonction de coût jusqu'à la même valeur qu'en d) et on obtient donc de moins bonnes performances dans ce cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNNs with Keras\n",
    "\n",
    "Adapt the example in this website https://keras.io/examples/vision/mnist_convnet/ to our problem. To this end:\n",
    "- change the number of classes and the input size\n",
    "- remove the expand_dims(x_train, -1): it is not necessary to expand the dimensions since our input is 3-dimensional \n",
    "- you may need to transpose the labels vector\n",
    "- change the categorical cross-entropy to the binary cross entropy given that our problem is binary classification. \n",
    "- also change the softmax to sigmoid, the more appropriate activation function for binary data\n",
    "\n",
    "We can choose a single neuron output passed through sigmoid, and then set a threshold to choose the class, or use two neuron output and then perform a softmax.\n",
    "\n",
    "**2.1** Can you get the accuracy better than in our hand single-neuron model?Try different configurations and explain the changes you have made.\n",
    "\n",
    "**2.2** Compute the train and test loss and accuracy after the model has been trained.  What model parameters does the ``fit`` function retain?\n",
    "\n",
    "**2.3** How many parameters does the network have, explain where the number comes from.\n",
    "\n",
    "**2.4** What is the receptive field of the network https://distill.pub/2019/computing-receptive-fields/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "x_train, y_train, x_test, y_test, classes=load_dataset()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "num_classes = 1\n",
    "input_shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"sigmoid\"),\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comiple and fit\n",
    "batch_size = 209\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train loss:\", score[0])\n",
    "print(\"Train accuracy:\", score[1])\n",
    "print('')\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1)** Oui on arrive à obtenir des performances bien meilleures que pour le modèle précedent. Par exemple en passant `epochs` à 20 et en mettant `batch_size` au nombre de features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2)** La fonction fit prend en argument :\n",
    "\n",
    "* `x_train`\n",
    "* `y_train`\n",
    "* `batch_size` : le nombre d'échantillons à considérer pour les calculs du gradient\n",
    "* `epochs` : le nombre de fois où le modèle voit chaque feature\n",
    "* `validation_split` : le pourcentage de données utilisées pour la validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3)** Le modèle dispose de **31937** paramètres : \n",
    "\n",
    "* La première couche de convolution dispose de : $3 \\times 3 \\times 32 \\times 3 + 32 = 896$ paramètres\n",
    "* La deuxième couche de convolution dispose de : $3 \\times 3 \\times 64 \\times 32 + 64 = 18496$ paramètres\n",
    "* la couche dense comporte $12545$ paramètres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
