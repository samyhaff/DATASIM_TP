{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1op-CbyLuN4"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszt2RUHE7lW"
      },
      "source": [
        "# Node Classification with Graph Neural Networks\n",
        "\n",
        "[Previous: Introduction: Hands-on Graph Neural Networks](https://colab.research.google.com/drive/1D5Yl32FP3PX3LWJPj6KNutrJHpVZVLMo#scrollTo=NgcpV4rjAWy-)\n",
        "\n",
        "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
        "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
        "\n",
        "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
        "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
        "Two documents are connected if there exists a citation link between them.\n",
        "The task is to infer the category of each document (7 in total).\n",
        "\n",
        "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
        "We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imGrKO5YH11-"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# 1 - Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# 2 - Gather some statistics about the graph. (Add your code between the brackets)\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqWR0j_kIx67"
      },
      "source": [
        "Overall, this dataset is quite similar to the previously used [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) network.\n",
        "We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9.\n",
        "For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class).\n",
        "This results in a training node label rate of only 5%.\n",
        "\n",
        "In contrast to `KarateClub`, this graph holds the additional attributes `val_mask` and `test_mask`, which denotes which nodes should be used for validation and testing.\n",
        "Furthermore, we make use of **[data transformations](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-transforms) via `transform=NormalizeFeatures()`**.\n",
        "Transforms can be used to modify your input data before inputting them into a neural network, *e.g.*, for normalization or data augmentation.\n",
        "Here, we [row-normalize](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.NormalizeFeatures) the bag-of-words input feature vectors.\n",
        "\n",
        "We can further see that this network is undirected, and that there exists no isolated nodes (each document has at least one citation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IRdAELVKOl6"
      },
      "source": [
        "## Training a Multi-layer Perception Network (MLP)\n",
        "\n",
        "In theory, we should be able to infer the category of a document solely based on its content, *i.e.* its bag-of-words feature representation, without taking any relational information into account.\n",
        "\n",
        "Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afXwPCA3KNoC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(MLP, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "\n",
        "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        # x = torch.nn.Softmax(dim=-1)(x)\n",
        "        return x\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_PO9EEHL7J6"
      },
      "source": [
        "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n",
        "Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=16`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n",
        "\n",
        "Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n",
        "We again make use of the **cross entropy loss** and **Adam optimizer**.\n",
        "This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YgHcLXMLk4o"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion with crossEntropyLoss.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "# Add your code where needed\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x[data.test_mask])\n",
        "      predict = torch.argmax(out, dim=1)\n",
        "      truth = data.y[data.test_mask]\n",
        "      N = len(data.y[data.test_mask])\n",
        "      test_acc = sum(predict == truth) / N\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG4IKy9YOLGF"
      },
      "source": [
        "After training the model, we can call the `test` function to see how well our model performs on unseen labels.\n",
        "Here, we are interested in the accuracy of the model, *i.e.*, the ratio of correctly classified nodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBBCeLlAL0oL"
      },
      "outputs": [],
      "source": [
        "print(data.test_mask)\n",
        "\n",
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jjJOB-VO-cw"
      },
      "source": [
        "**Question**: Comment the results, why do you think the model behaves poorly? \n",
        "\n",
        "**Remark:** The model fails to incorporate an important bias into the model: **Cited papers are very likely related to the category of a document**.\n",
        "That is exactly where Graph Neural Networks come into play and can help to boost the performance of our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Réponse:**Ce modèle est peu performatn car il ne prend pas du tout en compte les relations entre les documents. Or il est probable que les citations soient liées à la catégorie des documents (ce que l'on cherche à prédire)."
      ],
      "metadata": {
        "id": "bT3hg0KVn9eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "....."
      ],
      "metadata": {
        "id": "nf68lZDHmeHe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWGw54wRd98"
      },
      "source": [
        "## Training a Graph Neural Network (GNN)\n",
        "\n",
        "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n",
        "\n",
        "Following-up on [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) module.\n",
        "To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
        "In contrast, a single linear layer is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
        "$$\n",
        "\n",
        "which does not make use of neighboring node information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmXWs1dKIzD8"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create a GCN with the number of hidden channels = 16 \n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, 7)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "         h = self.conv1(x, edge_index)\n",
        "         h = h.relu()\n",
        "         h = F.dropout(h, p=0.5, training=self.training)\n",
        "         h = self.conv2(h, edge_index)\n",
        "         # h = h.relu()\n",
        " \n",
        "         return h\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhO8QDgYf_Q8"
      },
      "source": [
        "Let's visualize the node embeddings of our **untrained** GCN network.\n",
        "For visualization, we make use of [**TSNE**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to embed our 7-dimensional node embeddings onto a 2D plane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntt9qVFXlk6A"
      },
      "outputs": [],
      "source": [
        "model = GCN(hidden_channels=16)\n",
        "model.eval()\n",
        "\n",
        "out = model(data.x, data.edge_index)\n",
        "visualize(out, color=data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpdscco5g6kG"
      },
      "source": [
        "\n",
        "**Question**: What do you observe ? Comment the results of T-SNE. \n",
        "\n",
        "The training and testing procedure is once again the same, but this time we make use of the node features `x` **and** the graph connectivity `edge_index` as input to our GCN model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Réponse :** On observe que pour ce modèle non entrainé, on ne peut rien distinguer de particulier à partir de la T-SNE. En effet, aucun cluster ne se dégage."
      ],
      "metadata": {
        "id": "IDetyunxo9An"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3TAi69zI1bO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)[data.test_mask]\n",
        "      predict = torch.argmax(out, dim=1)\n",
        "      truth = data.y[data.test_mask]\n",
        "      N = len(truth)\n",
        "      test_acc = sum(predict == truth) / N\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opBBGQHqg5ZO"
      },
      "source": [
        "After training the model, we can check its test accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zOh6IIeI3Op"
      },
      "outputs": [],
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "# .4f : is to restrict the numbers after the comma (float number with 4 values after the comma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhofzjaqhfY2"
      },
      "source": [
        "**There it is!**\n",
        "By simply swapping the linear layers with GCN layers, we can reach **81.5% of test accuracy**!\n",
        "\n",
        "**Question**: How do you explain this result ? Why does a GCN do better than a linear layer ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette fois on obtient de bien meilleurs résultats que précedemment. Ceci s'expliquer par le fait qu'on prend en compte les citations et donc les relations entre les différents noeuds, porteuses d'information utile pour la classification.\n"
      ],
      "metadata": {
        "id": "G9wTFXsWrhL4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r_VmGMukf5R"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the original graph (containing all nodes/edges) and show its embedding using t-SNE.\n",
        "# Verify by looking at the output embeddings of the trained model. It should produce a far better clustering of nodes of the same category.\n",
        "\n",
        "out = model(data.x, data.edge_index)\n",
        "visualize(out, color=data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x0v3OZt5FPo"
      },
      "source": [
        "**Question** Comment the results. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Réponse :** Cette fois, sur le modèle entrainé, on distingue clairement les clusters correspondant aux 7 classes. On note la présence de quelques erreurs."
      ],
      "metadata": {
        "id": "w8rVD3IpsnYf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paMH3_7ejSg4"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model's performance.\n",
        "In the next section, we will look into how GNNs can be used for the task of graph classification.\n",
        "\n",
        "[Next: Graph Classification with Graph Neural Networks](https://colab.research.google.com/drive/1WZnUL_tzGk6VKCYLuOSzNddXZCvYPj6Z#scrollTo=zn5U4EE6K86v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-q6Do4INLET"
      },
      "source": [
        "## (Optional) Exercises\n",
        "\n",
        "1. To achieve better model performance and to avoid overfitting, it is usually a good idea to select the best model based on an additional validation set.\n",
        "The `Cora` dataset provides a validation node set as `data.val_mask`, but we haven't used it yet.\n",
        "Can you modify the code to select and test the model with the highest validation performance?\n",
        "This should bring test performance to **82% accuracy**.\n",
        "\n",
        "2. How does `GCN` behave when increasing the hidden feature dimensionality or the number of layers?\n",
        "Does increasing the number of layers help at all?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Node_Classification_PyG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}